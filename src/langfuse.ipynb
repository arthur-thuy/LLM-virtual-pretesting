{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "import asyncio\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Construct the path to the backend directory\n",
    "backend_path = os.path.abspath(os.path.join(os.path.dirname(current_dir), 'backend'))\n",
    "\n",
    "# Add the backend directory to PYTHONPATH\n",
    "if backend_path not in sys.path:\n",
    "    sys.path.insert(0, backend_path)\n",
    "    os.environ['PYTHONPATH'] = backend_path + os.pathsep + os.environ.get('PYTHONPATH', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import Langfuse\n",
    "from src.utils.llm import TextAnalyzer, PromptManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACE_TAGS = [\"eval\"] # tags to add to trace for filtering\n",
    "TRACE_USER_ID=\"dev\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse = Langfuse()\n",
    "prompt_manager = PromptManager(langfuse) # load latest version of prompts\n",
    "llm = TextAnalyzer(prompt_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_function(input, expected_output, output) :\n",
    "    result = {}\n",
    "    result[\"value\"] = 1 if output == expected_output else 0\n",
    "    result[\"comment\"] = \"\" # optional, useful to add reasoning\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_evals(\n",
    "    prompt_name,\n",
    "    dataset_name,\n",
    "    score_name,\n",
    "    run_name,\n",
    "    prompt_label=\"latest\",\n",
    "    run_description=\"\",\n",
    "    max_parallel=5\n",
    "):\n",
    "    prompt_manager.load_prompt_templates(prompt_label=prompt_label)\n",
    "    dataset = langfuse.get_dataset(dataset_name)\n",
    "    run_name = run_name + \"_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    semaphore = asyncio.Semaphore(max_parallel)\n",
    "\n",
    "    async def process_item(item):\n",
    "        async with semaphore:\n",
    "            trace_id = str(uuid.uuid4())\n",
    "            generation_id = str(uuid.uuid4())\n",
    "            text = item.input\n",
    "            \n",
    "            output = await llm.analyze_single_feature(\n",
    "                feature=prompt_name,\n",
    "                prompt_templates=prompt_manager.prompts,\n",
    "                text=text,\n",
    "                trace_id=trace_id,\n",
    "                trace_user_id=TRACE_USER_ID,\n",
    "                generation_id=generation_id,\n",
    "                tags=TRACE_TAGS,\n",
    "            )\n",
    "\n",
    "            item.link(\n",
    "                trace_or_observation=None,\n",
    "                run_name=run_name,\n",
    "                run_description=run_description,\n",
    "                trace_id=trace_id,\n",
    "                observation_id=generation_id,\n",
    "            )\n",
    "            \n",
    "            evaluation_result = evaluation_function(\n",
    "                item.input,\n",
    "                item.expected_output,\n",
    "                output,\n",
    "            )\n",
    "            \n",
    "            langfuse.score(\n",
    "                name=score_name,\n",
    "                value=evaluation_result[\"value\"], \n",
    "                trace_id=trace_id,\n",
    "                observation_id=generation_id,\n",
    "                comment=evaluation_result[\"comment\"],\n",
    "            )\n",
    "\n",
    "    tasks = [process_item(item) for item in dataset.items]\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "    # Flush the langfuse client to ensure all data is sent to the server at the end of the experiment run\n",
    "    langfuse.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await run_evals(\n",
    "    prompt_name=\"sarcasm\",    # \"sarcasm\", \"genre\", \"manipulation\" etc.\n",
    "    dataset_name=\"sarcasm\",   # Dataset name in langfuse\n",
    "    score_name=\"accuracy\",    # Score name in langfuse\n",
    "    run_name=\"run\",           # Any meaningful name for experiment\n",
    "    prompt_label=\"latest\",    # \"latest\" or \"production\"\n",
    "    run_description=\"\",       # Optional, useful to add details\n",
    "    max_parallel=5            # Max parallel requests, defaults to 5\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}