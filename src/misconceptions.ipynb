{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from typing import Callable\n",
    "\n",
    "import dotenv\n",
    "import pandas as pd\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    FewShotChatMessagePromptTemplate,\n",
    "    PromptTemplate,\n",
    ")\n",
    "from langchain_ollama import ChatOllama\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "\n",
    "from example_selector.example_selector import RandomExampleSelector\n",
    "from data_loader.data_loader import DataLoader\n",
    "from tools.constants import SILVER_DIR, TRAIN, VALIDATION, TEST, BRONZE_DIR\n",
    "\n",
    "# Reload the variables in your '.env' file (override the existing variables)\n",
    "dotenv.load_dotenv(\"../.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_format_input(row) -> str:\n",
    "    # NOTE: this is flexible wrt the number of answer options\n",
    "    text = f\"Question:\\n{row.q_text}\\n\\nOptions:\\n\"\n",
    "    for i, option in enumerate(row.options_text):\n",
    "        text += f\"{i+1}. {option}\\n\"\n",
    "    text += f\"\\nCorrect answer: {row.correct_answer}\"\n",
    "    return text\n",
    "\n",
    "\n",
    "def human_format_output(row) -> str:\n",
    "    return f\"Student answer: {row.student_answer}\"\n",
    "\n",
    "\n",
    "def apply_prompt_fmt(\n",
    "    df: pd.DataFrame, input_fmt: Callable, output_fmt: Callable\n",
    ") -> pd.DataFrame:\n",
    "    df_out = pd.DataFrame()\n",
    "    df_out[\"input\"] = df.apply(input_fmt, axis=1)\n",
    "    df_out[\"output\"] = df.apply(output_fmt, axis=1)\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data_loader = DataLoader(read_dir=SILVER_DIR, dataset_name=\"dbe_kt22\")\n",
    "dataset = data_loader.split_data(train_size=0.6, test_size=0.25, seed=42)\n",
    "\n",
    "# dataframes\n",
    "df_train = apply_prompt_fmt(dataset[TRAIN], human_format_input, human_format_output)\n",
    "df_val = apply_prompt_fmt(dataset[VALIDATION], human_format_input, human_format_output)\n",
    "df_test = apply_prompt_fmt(dataset[TEST], human_format_input, human_format_output)\n",
    "\n",
    "# list of dicts\n",
    "list_train = [{\"input\": row[\"input\"], \"output\": row[\"output\"]} for _, row in df_train.iterrows()]\n",
    "list_val = [{\"input\": row[\"input\"], \"output\": row[\"output\"]} for _, row in df_val.iterrows()]\n",
    "list_test = [{\"input\": row[\"input\"], \"output\": row[\"output\"]} for _, row in df_test.iterrows()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic few-shot prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create example selector\n",
    "\n",
    "NOTE: I need OpenAI credits to use the OpenAI embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples = few_shot_list\n",
    "# to_vectorize = [\" \".join(example.values()) for example in examples]\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "# vectorstore = Chroma.from_texts(to_vectorize, embeddings, metadatas=examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_selector = SemanticSimilarityExampleSelector(\n",
    "#     vectorstore=vectorstore,\n",
    "#     k=2,\n",
    "# )\n",
    "\n",
    "# # The prompt template will load examples by passing the input do the `select_examples` method\n",
    "# example_selector.select_examples({\"input\": \"horse\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the selector with k=3 for 3-shot prompting\n",
    "example_selector = RandomExampleSelector(examples=list_train, k=3)\n",
    "example_selector.select_examples({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_template = PromptTemplate.from_template(\n",
    "    \"You are a student working on {exam_type}, containing multiple choice questions. \"\n",
    "    \"You are shown a set of questions that you answered earlier in the exam, together with the correct answers and your student answers. \"\n",
    "    \"Analyse your responses to the questions and identify the possible misconceptions that led to answering incorrectly. \"\n",
    "\n",
    "    \"Inspect the new question and think how you would answer it as a student. \"\n",
    "    \"If you answer incorrectly, explain which misconception leads to selecting that answer. \"\n",
    "    \"If you answer correctly, explain why you think the answer is correct. \"\n",
    "    \"Provide your answer as an integer in the range 1-4. \"\n",
    ")\n",
    "\n",
    "system_prompt_input = system_prompt_template.format(\n",
    "    exam_type=\"a database systems exam (Department of Computer Science)\",\n",
    ")\n",
    "system_prompt_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic\n",
    "class MCQAnswer(BaseModel):\n",
    "    \"\"\"Answer to a multiple-choice question.\"\"\"\n",
    "\n",
    "    explanation: str = Field(\n",
    "        description=\"Misconception if incorrectly answered; motivation if correctly answered\"\n",
    "    )\n",
    "    student_answer: int = Field(\n",
    "        description=\"The student's answer to the question, as an integer (1-4)\"\n",
    "    )\n",
    "    # difficulty: str = Field(description=\"The difficulty level of the question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the few-shot prompt.\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    # The input variables select the values to pass to the example_selector\n",
    "    input_variables=[\"input\"],\n",
    "    example_selector=example_selector,\n",
    "    # Define how each example will be formatted.\n",
    "    # In this case, each example will become 2 messages:\n",
    "    # 1 human, and 1 AI\n",
    "    example_prompt=ChatPromptTemplate.from_messages(\n",
    "        [(\"human\", \"{input}\"), (\"ai\", \"{output}\")]\n",
    "    ),\n",
    ")\n",
    "\n",
    "out = few_shot_prompt.invoke(input=list_val[0][\"input\"]).to_messages()\n",
    "print(len(out))\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt_input),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(list_val[0][\"input\"])\n",
    "out = final_prompt.invoke(input=list_val[0][\"input\"]).to_messages()\n",
    "print(len(out))\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = ChatOllama(\n",
    "    model=\"llama3.2\",\n",
    "    temperature=0.5,\n",
    ").with_structured_output(MCQAnswer)\n",
    "chain = final_prompt | model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model\n",
    "val_example = list_val[0]\n",
    "val_output = chain.invoke(val_example[\"input\"])\n",
    "val_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add func to only print input (also printing output can be confusing)\n",
    "def print_example(example: dict) -> None:\n",
    "    \"\"\"Print single example.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    example : dict\n",
    "        Example dictionary with 'input' and 'output' keys.\n",
    "    \"\"\"\n",
    "    text = (\n",
    "        \"#\" * 40\n",
    "        + f\"\\nINPUT\\n\"\n",
    "        + \"#\" * 40\n",
    "        + f\"\\n{example['input']}\\n\"\n",
    "        + \"#\" * 40\n",
    "        + f\"\\nOUTPUT\\n\"\n",
    "        + \"#\" * 40\n",
    "        + f\"\\n{example['output']}\\n\"\n",
    "    )\n",
    "    print(text)\n",
    "\n",
    "\n",
    "print_example(list_val[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_virtual_pretesting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
