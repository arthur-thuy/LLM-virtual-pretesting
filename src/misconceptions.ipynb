{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library imports\n",
    "import os\n",
    "import random\n",
    "from typing import Callable\n",
    "\n",
    "# related third party imports\n",
    "import dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import structlog\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    FewShotChatMessagePromptTemplate,\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_ollama import ChatOllama\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from yacs.config import CfgNode\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# local application/library specific imports\n",
    "from example_selector.example_selector import (\n",
    "    RandomExampleSelector,\n",
    "    StudentIDExampleSelector,\n",
    ")\n",
    "from data_loader.data_loader import DataLoader\n",
    "from tools.constants import SILVER_DIR, TRAIN, VALIDATION, TEST, MODEL_STRUCTURED_OUTPUT\n",
    "from prompt.few_shot_prompt import (\n",
    "    df_to_listdict,\n",
    "    human_format_input,\n",
    "    human_format_output,\n",
    "    apply_prompt_fmt,\n",
    ")\n",
    "from model.build import build_model\n",
    "\n",
    "logger = structlog.get_logger()\n",
    "\n",
    "# Reload the variables in your '.env' file (override the existing variables)\n",
    "dotenv.load_dotenv(\"../.env\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INPUTS ###\n",
    "MODEL_NAME = \"olmo2:7b\"  # \"gpt-4o-mini\"  # \"llama3\"  # \"llama3.2\"\n",
    "MODEL_PROVIDER = \"ollama\"  # \"openai\"  # \n",
    "SUPPORTS_STRUCTURED_OUTPUT = MODEL_STRUCTURED_OUTPUT[MODEL_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cfg = CfgNode(\n",
    "    {\n",
    "        \"NAME\": MODEL_NAME,\n",
    "        \"PROVIDER\": MODEL_PROVIDER,\n",
    "        \"TEMPERATURE\": 0.5,\n",
    "        \"FORMAT\": \"json\",\n",
    "        \"MAX_TOKENS\": None,\n",
    "        \"TIMEOUT\": None,\n",
    "        \"MAX_RETRIES\": None,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data_loader = DataLoader(read_dir=SILVER_DIR, dataset_name=\"dbe_kt22\")\n",
    "dataset = data_loader.split_data(train_size=0.6, test_size=0.25, seed=42)\n",
    "\n",
    "# dataframes\n",
    "df_train = apply_prompt_fmt(\n",
    "    df=dataset[TRAIN], input_fmt=human_format_input, output_fmt=human_format_output\n",
    ")\n",
    "df_val = apply_prompt_fmt(\n",
    "    df=dataset[VALIDATION], input_fmt=human_format_input, output_fmt=human_format_output\n",
    ")\n",
    "df_test = apply_prompt_fmt(\n",
    "    df=dataset[TEST], input_fmt=human_format_input, output_fmt=human_format_output\n",
    ")\n",
    "\n",
    "# list of dicts\n",
    "list_train = df_to_listdict(df_train)\n",
    "list_val = df_to_listdict(df_val)\n",
    "list_test = df_to_listdict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val.iloc[:10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic few-shot prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create example selector\n",
    "\n",
    "NOTE: I need OpenAI credits to use the OpenAI embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples = few_shot_list\n",
    "# to_vectorize = [\" \".join(example.values()) for example in examples]\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "# vectorstore = Chroma.from_texts(to_vectorize, embeddings, metadatas=examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_selector = SemanticSimilarityExampleSelector(\n",
    "#     vectorstore=vectorstore,\n",
    "#     k=2,\n",
    "# )\n",
    "\n",
    "# # The prompt template will load examples by passing the input do the `select_examples` method\n",
    "# example_selector.select_examples({\"input\": \"horse\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the selector with k=3 for 3-shot prompting\n",
    "example_selector = RandomExampleSelector(examples=list_train, k=3)\n",
    "example_selector.select_examples({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Select examples of a specific student\n",
    "# example_selector = StudentIDExampleSelector(examples=list_train, k=3)\n",
    "# example_selector.select_examples({\"student_id\": 395})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic\n",
    "class MCQAnswer(BaseModel):\n",
    "    \"\"\"Answer to a multiple-choice question.\"\"\"\n",
    "\n",
    "    explanation: str = Field(\n",
    "        description=\"Misconception if incorrectly answered; motivation if correctly answered\"\n",
    "    )\n",
    "    student_answer: int = Field(\n",
    "        description=\"The student's answer to the question, as an integer (1-4)\"\n",
    "    )\n",
    "    # difficulty: str = Field(description=\"The difficulty level of the question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the few-shot prompt.\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    # The input variables select the values to pass to the example_selector\n",
    "    input_variables=[\"student_id\"],  # TODO: do not hardcode\n",
    "    example_selector=example_selector,\n",
    "    # Define how each example will be formatted.\n",
    "    # In this case, each example will become 2 messages:\n",
    "    # 1 human, and 1 AI\n",
    "    example_prompt=ChatPromptTemplate.from_messages(\n",
    "        [(\"human\", \"{input}\"), (\"ai\", \"{output}\")]\n",
    "    ),\n",
    ")\n",
    "\n",
    "out = few_shot_prompt.invoke(input=list_val[0]).to_messages()\n",
    "print(len(out))\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_raw = (\n",
    "    \"You are a student working on {exam_type}, containing multiple choice questions. \"\n",
    "    \"You are shown a set of questions that you answered earlier in the exam, together with the correct answers and your student answers. \"\n",
    "    \"Analyse your responses to the questions and identify the possible misconceptions that led to answering incorrectly. \"\n",
    "    \"Inspect the new question and think how you would answer it as a student. \"\n",
    "    \"If you answer incorrectly, explain which misconception leads to selecting that answer. \"\n",
    "    \"If you answer correctly, explain why you think the answer is correct. \"\n",
    "    \"Provide your answer as an integer in the range 1-4. \"\n",
    ")\n",
    "# Set up a parser (not used if model supports structured output)\n",
    "parser = PydanticOutputParser(pydantic_object=MCQAnswer)\n",
    "if not SUPPORTS_STRUCTURED_OUTPUT:\n",
    "    system_prompt_raw += \"Wrap the output in `json` tags\\n{format_instructions}\"\n",
    "\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt_raw),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ").partial(\n",
    "    format_instructions=parser.get_format_instructions(),\n",
    "    exam_type=\"a database systems exam (Department of Computer Science)\",\n",
    ")\n",
    "\n",
    "# print(\n",
    "#     final_prompt.invoke(\n",
    "#         input=list_val[0],\n",
    "#     ).to_string()\n",
    "# )\n",
    "out = final_prompt.invoke(input=list_val[0]).to_messages()\n",
    "print(len(out))\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = build_model(model_cfg=model_cfg)\n",
    "if SUPPORTS_STRUCTURED_OUTPUT:\n",
    "    model = model.with_structured_output(MCQAnswer, include_raw=True)\n",
    "\n",
    "# chain\n",
    "chain = final_prompt | model\n",
    "# if not SUPPORTS_STRUCTURED_OUTPUT:\n",
    "#     chain = chain.pipe(parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run model\n",
    "# val_example = list_val[0]\n",
    "# val_output = chain.invoke(val_example)\n",
    "# val_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model in batch\n",
    "val_output = chain.batch(list_val[:10])\n",
    "val_output = [output[\"raw\"] for output in val_output]\n",
    "val_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_output_test = val_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages.ai import AIMessage\n",
    "\n",
    "val_output_test[0] = AIMessage(content=\"\"\"{\"explanation\": \"I don't know\", \"student_answer\": \"bla\"}\"\"\")\n",
    "val_output_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.exceptions import OutputParserException\n",
    "\n",
    "def validate_output(outputs: list, schema) -> list:\n",
    "    \"\"\"Validate the LLM outputs against the schema.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    outputs : list\n",
    "        List of AIMessages\n",
    "    schema : _type_\n",
    "        Pydanctic schema\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List of validated outputs\n",
    "    \"\"\"\n",
    "    outputs_validated = []\n",
    "    for i, output in enumerate(outputs):\n",
    "        try:\n",
    "            output_validated = parser.invoke(output)\n",
    "        except OutputParserException as e:\n",
    "            logger.warning(\"Invalid output\", index=i)\n",
    "            print(e)\n",
    "            output_validated = schema(explanation=\"\", student_answer=-1)\n",
    "        outputs_validated.append(output_validated)\n",
    "\n",
    "    return outputs_validated\n",
    "\n",
    "# val_output_validated = validate_output(outputs=val_output, schema=MCQAnswer)\n",
    "# val_output_validated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_output_validated = validate_output(outputs=val_output_test, schema=MCQAnswer)\n",
    "val_output_validated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: count number of invalid responses (student_answer=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred = np.array([output.student_answer for output in val_output_validated])\n",
    "y_val_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_student = dataset[VALIDATION][\"student_answer\"].to_numpy()[:10]\n",
    "y_val_student\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_true = dataset[VALIDATION][\"correct_answer\"].to_numpy()[:10]\n",
    "y_val_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_student_pred = accuracy_score(y_true=y_val_student, y_pred=y_val_pred)\n",
    "acc_true_student = accuracy_score(y_true=y_val_true, y_pred=y_val_student)\n",
    "acc_true_pred = accuracy_score(y_true=y_val_true, y_pred=y_val_pred)\n",
    "\n",
    "print(f\"{acc_student_pred = }\")\n",
    "print(f\"{acc_true_student = }\")\n",
    "print(f\"{acc_true_pred = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add func to only print input (also printing output can be confusing)\n",
    "def print_example(example: dict) -> None:\n",
    "    \"\"\"Print single example.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    example : dict\n",
    "        Example dictionary with 'input' and 'output' keys.\n",
    "    \"\"\"\n",
    "    text = (\n",
    "        \"#\" * 40\n",
    "        + f\"\\nINPUT\\n\"\n",
    "        + \"#\" * 40\n",
    "        + f\"\\n{example['input']}\\n\"\n",
    "        + \"#\" * 40\n",
    "        + f\"\\nOUTPUT\\n\"\n",
    "        + \"#\" * 40\n",
    "        + f\"\\n{example['output']}\\n\"\n",
    "    )\n",
    "    print(text)\n",
    "\n",
    "\n",
    "print_example(list_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_virtual_pretesting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
