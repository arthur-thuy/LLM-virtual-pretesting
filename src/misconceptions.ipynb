{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library imports\n",
    "import os\n",
    "import random\n",
    "from typing import Callable\n",
    "\n",
    "# related third party imports\n",
    "import dotenv\n",
    "import pandas as pd\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    FewShotChatMessagePromptTemplate,\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_ollama import ChatOllama\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from yacs.config import CfgNode\n",
    "\n",
    "# local application/library specific imports\n",
    "from example_selector.example_selector import (\n",
    "    RandomExampleSelector,\n",
    "    StudentIDExampleSelector,\n",
    ")\n",
    "from data_loader.data_loader import DataLoader\n",
    "from tools.constants import SILVER_DIR, TRAIN, VALIDATION, TEST\n",
    "from prompt_template.example_prompt import (\n",
    "    df_to_listdict,\n",
    "    human_format_input,\n",
    "    human_format_output,\n",
    "    apply_prompt_fmt,\n",
    ")\n",
    "from model.build import build_model\n",
    "\n",
    "# Reload the variables in your '.env' file (override the existing variables)\n",
    "dotenv.load_dotenv(\"../.env\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_STRUCTURED_OUTPUT = {\n",
    "    \"llama3\": False,\n",
    "    \"llama3.2\": True,\n",
    "    \"olmo2\": False,\n",
    "    \"gpt-4\": True,\n",
    "    \"gpt-4o-mini\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INPUTS ###\n",
    "MODEL_NAME = \"gpt-4o-mini\"  # \"llama3\"  # \"llama3.2\"\n",
    "MODEL_PROVIDER = \"openai\"  # \"ollama\"\n",
    "SUPPORTS_STRUCTURED_OUTPUT = MODEL_STRUCTURED_OUTPUT[MODEL_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cfg = CfgNode({\"NAME\": MODEL_NAME, \"PROVIDER\": MODEL_PROVIDER, \"TEMPERATURE\": 0.5, \"MAX_TOKENS\": None, \"TIMEOUT\": None, \"MAX_RETRIES\": None})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data_loader = DataLoader(read_dir=SILVER_DIR, dataset_name=\"dbe_kt22\")\n",
    "dataset = data_loader.split_data(train_size=0.6, test_size=0.25, seed=42)\n",
    "\n",
    "# dataframes\n",
    "df_train = apply_prompt_fmt(\n",
    "    df=dataset[TRAIN], input_fmt=human_format_input, output_fmt=human_format_output\n",
    ")\n",
    "df_val = apply_prompt_fmt(\n",
    "    df=dataset[VALIDATION], input_fmt=human_format_input, output_fmt=human_format_output\n",
    ")\n",
    "df_test = apply_prompt_fmt(\n",
    "    df=dataset[TEST], input_fmt=human_format_input, output_fmt=human_format_output\n",
    ")\n",
    "\n",
    "# list of dicts\n",
    "list_train = df_to_listdict(df_train)\n",
    "list_val = df_to_listdict(df_val)\n",
    "list_test = df_to_listdict(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic few-shot prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create example selector\n",
    "\n",
    "NOTE: I need OpenAI credits to use the OpenAI embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples = few_shot_list\n",
    "# to_vectorize = [\" \".join(example.values()) for example in examples]\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "# vectorstore = Chroma.from_texts(to_vectorize, embeddings, metadatas=examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_selector = SemanticSimilarityExampleSelector(\n",
    "#     vectorstore=vectorstore,\n",
    "#     k=2,\n",
    "# )\n",
    "\n",
    "# # The prompt template will load examples by passing the input do the `select_examples` method\n",
    "# example_selector.select_examples({\"input\": \"horse\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the selector with k=3 for 3-shot prompting\n",
    "# example_selector = RandomExampleSelector(examples=list_train, k=3)\n",
    "# example_selector.select_examples({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select examples of a specific student\n",
    "example_selector = StudentIDExampleSelector(examples=list_train, k=3)\n",
    "example_selector.select_examples({\"student_id\": 395})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic\n",
    "class MCQAnswer(BaseModel):\n",
    "    \"\"\"Answer to a multiple-choice question.\"\"\"\n",
    "\n",
    "    explanation: str = Field(\n",
    "        description=\"Misconception if incorrectly answered; motivation if correctly answered\"\n",
    "    )\n",
    "    student_answer: int = Field(\n",
    "        description=\"The student's answer to the question, as an integer (1-4)\"\n",
    "    )\n",
    "    # difficulty: str = Field(description=\"The difficulty level of the question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_raw = (\n",
    "    \"You are a student working on {exam_type}, containing multiple choice questions. \"\n",
    "    \"You are shown a set of questions that you answered earlier in the exam, together with the correct answers and your student answers. \"\n",
    "    \"Analyse your responses to the questions and identify the possible misconceptions that led to answering incorrectly. \"\n",
    "    \"Inspect the new question and think how you would answer it as a student. \"\n",
    "    \"If you answer incorrectly, explain which misconception leads to selecting that answer. \"\n",
    "    \"If you answer correctly, explain why you think the answer is correct. \"\n",
    "    \"Provide your answer as an integer in the range 1-4. \"\n",
    ")\n",
    "if SUPPORTS_STRUCTURED_OUTPUT:\n",
    "    system_prompt_template = PromptTemplate.from_template(system_prompt_raw)\n",
    "else:\n",
    "    system_prompt_raw += \"Wrap the output in `json` tags\\n{format_instructions}\"\n",
    "    # Set up a parser\n",
    "    parser = PydanticOutputParser(pydantic_object=MCQAnswer)\n",
    "    system_prompt_template = PromptTemplate.from_template(system_prompt_raw).partial(\n",
    "        format_instructions=parser.get_format_instructions()\n",
    "    )\n",
    "\n",
    "system_prompt_input = system_prompt_template.format(\n",
    "    exam_type=\"a database systems exam (Department of Computer Science)\",\n",
    ")\n",
    "system_prompt_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: problem if SUPPORTS_STRUCTURED_OUTPUT is False\n",
    "\n",
    "# Error: Note: if you intended {\"description\"} to be part of the string and not a variable, please escape it with double curly braces like: \\'{{\"description\"}}\\'\n",
    "# -> how do I do this???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: check if this works\n",
    "\n",
    "# system_prompt_raw = (\n",
    "#     \"You are a student working on {exam_type}, containing multiple choice questions. \"\n",
    "#     \"You are shown a set of questions that you answered earlier in the exam, together with the correct answers and your student answers. \"\n",
    "#     \"Analyse your responses to the questions and identify the possible misconceptions that led to answering incorrectly. \"\n",
    "#     \"Inspect the new question and think how you would answer it as a student. \"\n",
    "#     \"If you answer incorrectly, explain which misconception leads to selecting that answer. \"\n",
    "#     \"If you answer correctly, explain why you think the answer is correct. \"\n",
    "#     \"Provide your answer as an integer in the range 1-4. \"\n",
    "#     \"Wrap the output in `json` tags\\n{format_instructions}\"\n",
    "# )\n",
    "\n",
    "# prompt = PromptTemplate(\n",
    "#     template=system_prompt_raw,\n",
    "#     input_variables=[],\n",
    "#     partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    "# )\n",
    "# system_prompt_template = SystemMessagePromptTemplate(prompt=prompt)\n",
    "# system_prompt_template.format(\n",
    "#     exam_type=\"a database systems exam (Department of Computer Science)\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the few-shot prompt.\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    # The input variables select the values to pass to the example_selector\n",
    "    input_variables=[\"student_id\"],\n",
    "    example_selector=example_selector,\n",
    "    # Define how each example will be formatted.\n",
    "    # In this case, each example will become 2 messages:\n",
    "    # 1 human, and 1 AI\n",
    "    example_prompt=ChatPromptTemplate.from_messages(\n",
    "        [(\"human\", \"{input}\"), (\"ai\", \"{output}\")]\n",
    "    ),\n",
    ")\n",
    "\n",
    "out = few_shot_prompt.invoke(input=list_val[0]).to_messages()\n",
    "print(len(out))\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt_input),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(list_val[0])\n",
    "out = final_prompt.invoke(input=list_val[0]).to_messages()\n",
    "print(len(out))\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model_raw = build_model(model_cfg=model_cfg)\n",
    "if SUPPORTS_STRUCTURED_OUTPUT:\n",
    "    model = model_raw.with_structured_output(MCQAnswer)\n",
    "else:\n",
    "    model = model_raw\n",
    "\n",
    "# chain\n",
    "chain = final_prompt | model\n",
    "if not SUPPORTS_STRUCTURED_OUTPUT:\n",
    "    chain = chain.with_output_parser(parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model\n",
    "val_example = list_val[0]\n",
    "val_output = chain.invoke(val_example)\n",
    "val_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add func to only print input (also printing output can be confusing)\n",
    "def print_example(example: dict) -> None:\n",
    "    \"\"\"Print single example.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    example : dict\n",
    "        Example dictionary with 'input' and 'output' keys.\n",
    "    \"\"\"\n",
    "    text = (\n",
    "        \"#\" * 40\n",
    "        + f\"\\nINPUT\\n\"\n",
    "        + \"#\" * 40\n",
    "        + f\"\\n{example['input']}\\n\"\n",
    "        + \"#\" * 40\n",
    "        + f\"\\nOUTPUT\\n\"\n",
    "        + \"#\" * 40\n",
    "        + f\"\\n{example['output']}\\n\"\n",
    "    )\n",
    "    print(text)\n",
    "\n",
    "\n",
    "print_example(list_val[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually implementing structured output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set up a parser\n",
    "# parser = PydanticOutputParser(pydantic_object=MCQAnswer)\n",
    "\n",
    "# # Prompt\n",
    "# prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\n",
    "#             \"system\",\n",
    "#             \"Answer the user query. Wrap the output in `json` tags\\n{format_instructions}\",\n",
    "#         ),\n",
    "#         (\"human\", \"{query}\"),\n",
    "#     ]\n",
    "# ).partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "# query = \"Anna is 23 years old and she is 6 feet tall\"\n",
    "\n",
    "# print(prompt.invoke({\"query\": query}).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain = prompt | model | parser\n",
    "\n",
    "# chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import List\n",
    "\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain.chat_models import init_chat_model\n",
    "# from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# class Person(BaseModel):\n",
    "#     \"\"\"Information about a person.\"\"\"\n",
    "\n",
    "#     name: str = Field(..., description=\"The name of the person\")\n",
    "#     height_in_meters: float = Field(\n",
    "#         ..., description=\"The height of the person expressed in meters.\"\n",
    "#     )\n",
    "\n",
    "\n",
    "# class People(BaseModel):\n",
    "#     \"\"\"Identifying information about all people in a text.\"\"\"\n",
    "\n",
    "#     people: List[Person]\n",
    "\n",
    "\n",
    "# # Prompt\n",
    "# prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\n",
    "#             \"system\",\n",
    "#             \"Answer the user query.\",\n",
    "#         ),\n",
    "#         (\"human\", \"{query}\"),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# query = \"Anna is 23 years old and she is 6 feet tall\"\n",
    "\n",
    "# print(prompt.invoke({\"query\": query}).to_string())\n",
    "\n",
    "# model_name = \"llama3.2\"  # \"gpt-4o-mini\"\n",
    "# model_provider = \"ollama\"  # \"openai\"\n",
    "# llm = init_chat_model(model_name, model_provider=model_provider)\n",
    "# structured_llm = llm.with_structured_output(People)\n",
    "# chain = prompt | structured_llm\n",
    "\n",
    "# chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import List\n",
    "\n",
    "# from langchain_core.output_parsers import PydanticOutputParser\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain.chat_models import init_chat_model\n",
    "# from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# class Person(BaseModel):\n",
    "#     \"\"\"Information about a person.\"\"\"\n",
    "\n",
    "#     name: str = Field(..., description=\"The name of the person\")\n",
    "#     height_in_meters: float = Field(\n",
    "#         ..., description=\"The height of the person expressed in meters.\"\n",
    "#     )\n",
    "\n",
    "\n",
    "# class People(BaseModel):\n",
    "#     \"\"\"Identifying information about all people in a text.\"\"\"\n",
    "\n",
    "#     people: List[Person]\n",
    "\n",
    "\n",
    "# # Set up a parser\n",
    "# parser = PydanticOutputParser(pydantic_object=People)\n",
    "\n",
    "# # Prompt\n",
    "# prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\n",
    "#             \"system\",\n",
    "#             \"Answer the user query. Wrap the output in `json` tags\\n{format_instructions}\",\n",
    "#         ),\n",
    "#         (\"human\", \"{query}\"),\n",
    "#     ]\n",
    "# ).partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "# query = \"Anna is 23 years old and she is 6 feet tall\"\n",
    "\n",
    "# print(prompt.invoke({\"query\": query}).to_string())\n",
    "\n",
    "# model_name = \"llama3\"  # \"llama3.2\"  # \"gpt-4o-mini\"\n",
    "# model_provider = \"ollama\"  # \"openai\"\n",
    "# llm = init_chat_model(model_name, model_provider=model_provider)\n",
    "# chain = prompt | llm | parser\n",
    "\n",
    "# chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_virtual_pretesting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
