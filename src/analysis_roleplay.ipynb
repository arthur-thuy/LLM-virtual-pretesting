{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library imports\n",
    "# /\n",
    "\n",
    "# related third party imports\n",
    "import structlog\n",
    "\n",
    "# local application/library specific imports\n",
    "from tools.configurator import (\n",
    "    get_configs_out,\n",
    "    get_config_ids,\n",
    ")\n",
    "from tools.analyzer import (\n",
    "    print_table_from_dict,\n",
    "    print_df_from_dict,\n",
    "    get_results_dict,\n",
    "    merge_all_results,\n",
    "    create_config_id_print,\n",
    "    get_config_df,\n",
    "    check_overlap,\n",
    ")\n",
    "# from tools.plotter import plot_level_correctness\n",
    "\n",
    "\n",
    "logger = structlog.get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### INPUTS #####\n",
    "EXP_NAME = (\n",
    "    # context\n",
    "    # \"roleplay_miscon_test_kt_20250820-204306\"\n",
    "\n",
    "    # no context\n",
    "    # \"roleplay_miscon_test_kt_nocontext_20250821-075331\"\n",
    "\n",
    "    # merged\n",
    "    # \"roleplay_miscon_test_kt_merged_20250821\"\n",
    "\n",
    "    # CFE\n",
    "    \"roleplay_cupacfe_val_20250919-112843\"\n",
    ")\n",
    "SPLIT = \"val\" if \"val\" in EXP_NAME else \"test\"\n",
    "EXCLUDE_METRICS = [\n",
    "    \"val_acc_true_pred\",\n",
    "    \"val_f1_true_pred\",\n",
    "]\n",
    "LEGEND_EXACT = True\n",
    "PROBLEM_TYPE = \"roleplay\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC2LEGEND_DICT = {\n",
    "    f\"{SPLIT}_rmse\": f\"{SPLIT} RMSE\",\n",
    "    f\"{SPLIT}_mae\": f\"{SPLIT} MAE\",\n",
    "    f\"{SPLIT}_llm_correctness\": \"llm correctness\",\n",
    "    f\"{SPLIT}_monotonicity\": f\"{SPLIT} monotonicity\",\n",
    "    f\"{SPLIT}_prop_invalid\": f\"{SPLIT} prop invalid\",\n",
    "    f\"{SPLIT}_distractor_alignment\": f\"{SPLIT} distr alignment\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = get_configs_out(EXP_NAME)\n",
    "config_ids = get_config_ids(configs, problem_type=PROBLEM_TYPE)\n",
    "config_dict = {config_id: cfg for config_id, cfg in zip(config_ids, configs)}\n",
    "\n",
    "CONFIG2LEGEND_DICT = {\n",
    "    config_id: create_config_id_print(config_id) for config_id in config_ids\n",
    "}\n",
    "legend_kwargs = {\n",
    "    \"config2legend\": CONFIG2LEGEND_DICT,\n",
    "    \"legend_exact\": LEGEND_EXACT,\n",
    "    \"metric2legend\": METRIC2LEGEND_DICT,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge results for all configs\n",
    "run_id_dict = merge_all_results(EXP_NAME, config_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Val/Test set performance\n",
    "## Complete table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = get_results_dict(\n",
    "    exp_name=EXP_NAME,\n",
    "    config_ids=config_ids,\n",
    "    run_id=None,\n",
    ")\n",
    "# # NOTE: print paper-like table with this code\n",
    "# print_table_from_dict(\n",
    "#     eval_dict=results_dict,\n",
    "#     exp_name=EXP_NAME,\n",
    "#     exclude_metrics=EXCLUDE_METRICS,\n",
    "#     decimals=3,\n",
    "#     **legend_kwargs,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: print dataframe\n",
    "df = print_df_from_dict(\n",
    "    eval_dict=results_dict,\n",
    "    exp_name=EXP_NAME,\n",
    "    exclude_metrics=EXCLUDE_METRICS,\n",
    "    **legend_kwargs,\n",
    "    # save=True,\n",
    "    # save_kwargs={\"fname\": os.path.join(\"output\", EXP_NAME, \"results.csv\")},\n",
    ")\n",
    "\n",
    "df_config = get_config_df(config_dict)\n",
    "\n",
    "# mean\n",
    "df_mean = df.xs('mean', axis=1, level=1, drop_level=True)\n",
    "df_results = df_mean.merge(df_config, how=\"left\", on=\"config_id\")\n",
    "df_results = df_results.reindex(\n",
    "    columns=(\n",
    "        list(df_config.columns)\n",
    "        + list([a for a in df_mean.columns if a not in df_config.columns])\n",
    "    )\n",
    ")\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard error\n",
    "df_stderr = df.xs(\"stderr\", axis=1, level=1, drop_level=True)\n",
    "df_stderr = df_stderr.merge(df_config, how=\"left\", on=\"config_id\")\n",
    "df_stderr = df_stderr.reindex(\n",
    "    columns=(\n",
    "        list(df_config.columns)\n",
    "        + list([a for a in df_stderr.columns if a not in df_config.columns])\n",
    "    )\n",
    ")\n",
    "# df_stderr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"kt\" in EXP_NAME:\n",
    "    agg_dict = {\n",
    "        f\"{SPLIT} RMSE\": \"mean\",\n",
    "        f\"{SPLIT} MAE\": \"mean\",\n",
    "        f\"{SPLIT} monotonicity\": \"mean\",\n",
    "        \"llm correctness\": \"mean\",\n",
    "    }\n",
    "else:\n",
    "    agg_dict = {\n",
    "        f\"{SPLIT} RMSE\": \"mean\",\n",
    "        f\"{SPLIT} MAE\": \"mean\",\n",
    "        f\"{SPLIT} monotonicity\": \"mean\",\n",
    "        # f\"{SPLIT} distr alignment\": \"mean\",\n",
    "        \"llm correctness\": \"mean\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_context = df_results[df_results[\"num_examples\"] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect average performance per config value\n",
    "FEATURE = \"context\"\n",
    "df_results_context.groupby(FEATURE).agg(agg_dict).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect average performance per config value\n",
    "FEATURE = \"num_examples\"\n",
    "df_results_context.groupby(FEATURE).agg(agg_dict).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect average performance per config value\n",
    "FEATURE = \"model\"\n",
    "df_results_context.groupby(FEATURE).agg(agg_dict).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect average performance per config value\n",
    "FEATURE = \"temp\"\n",
    "df_results_context.groupby(FEATURE).agg(agg_dict).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect average performance per config value\n",
    "FEATURE = \"prompt\"\n",
    "df_results_context.groupby(FEATURE).agg(agg_dict).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect average performance per config value\n",
    "FEATURE = \"example_selec\"\n",
    "df_results_context.groupby(FEATURE).agg(agg_dict).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect average performance per config value\n",
    "FEATURE = \"struc_output\"\n",
    "df_results_context.groupby(FEATURE).agg(agg_dict).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the full rows for these best-performing configs\n",
    "metric = f\"{SPLIT} RMSE\"\n",
    "\n",
    "best_indices = df_results_context.groupby([\"model\"])[metric].idxmin()  # NOTE: min because RMSE\n",
    "best_configs = df_results_context.loc[best_indices]\n",
    "best_configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-contextual models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_nocontext = df_results[df_results[\"num_examples\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_nocontext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_results_nocontext.empty:\n",
    "    # inspect average performance per config value\n",
    "    FEATURE = \"context\"\n",
    "    display(df_results_nocontext.groupby(FEATURE).agg(agg_dict).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_results_nocontext.empty:\n",
    "    # inspect average performance per config value\n",
    "    FEATURE = \"num_examples\"\n",
    "    display(df_results_nocontext.groupby(FEATURE).agg(agg_dict).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_results_nocontext.empty:\n",
    "    # inspect average performance per config value\n",
    "    FEATURE = \"model\"\n",
    "    display(df_results_nocontext.groupby(FEATURE).agg(agg_dict).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_results_nocontext.empty:\n",
    "    # inspect average performance per config value\n",
    "    FEATURE = \"temp\"\n",
    "    display(df_results_nocontext.groupby(FEATURE).agg(agg_dict).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_results_nocontext.empty:\n",
    "    # inspect average performance per config value\n",
    "    FEATURE = \"prompt\"\n",
    "    display(df_results_nocontext.groupby(FEATURE).agg(agg_dict).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_results_nocontext.empty:\n",
    "    # inspect average performance per config value\n",
    "    FEATURE = \"example_selec\"\n",
    "    display(df_results_nocontext.groupby(FEATURE).agg(agg_dict).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_results_nocontext.empty:\n",
    "    # inspect average performance per config value\n",
    "    FEATURE = \"struc_output\"\n",
    "    display(df_results_nocontext.groupby(FEATURE).agg(agg_dict).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the full rows for these best-performing configs\n",
    "metric = f\"{SPLIT} RMSE\"\n",
    "\n",
    "best_indices = df_results_nocontext.groupby([\"model\"])[metric].idxmin()  # NOTE: min because RMSE\n",
    "best_configs = df_results_nocontext.loc[best_indices]\n",
    "best_configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual & Non-contextual models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metric = df[[f\"{SPLIT} monotonicity\", f\"{SPLIT} RMSE\"]].droplevel(0, axis=1)\n",
    "df_metric.columns = ['mean_monotonicity', 'stderr_monotonicity', \"mean_rmse\", \"stderr_rmse\"]\n",
    "df_metric = df_metric.merge(df_config, how=\"left\", on=\"config_id\")\n",
    "df_metric = df_metric.reindex(\n",
    "    columns=(\n",
    "        list(df_config.columns)\n",
    "        + list([a for a in df_metric.columns if a not in df_config.columns])\n",
    "    )\n",
    ")\n",
    "df_metric[\"performance monotonicity\"] = df_metric.apply(lambda x: f\"{x['mean_monotonicity']:.3f} \\gray{{$\\pm$ {x['stderr_monotonicity']:.3f}}}\", axis=1)\n",
    "df_metric[\"performance rmse\"] = df_metric.apply(lambda x: f\"{x['mean_rmse']:.3f} \\gray{{$\\pm$ {x['stderr_rmse']:.3f}}}\", axis=1)\n",
    "# extract model family and size\n",
    "df_metric[\"family\"] = df_metric[\"model\"].str.extract(r\"^(.*?):\")[0]\n",
    "df_metric[\"size\"] = (\n",
    "        df_metric[\"model\"].str.extract(r\":(\\d+\\.?\\d*)b$\")[0].astype(float).round(1)\n",
    "    )\n",
    "# create new column to group on having context or not\n",
    "context_map = {True: \"Context\", False: \"No context\"}\n",
    "df_metric[\"context\"] = df_metric[\"prompt\"].str.contains(\"_context\").map(context_map)\n",
    "\n",
    "df_metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_metric.sort_values(by=[\"family\", \"size\", \"context\"], ascending=True)[[\"family\", \"size\", \"context\", \"performance monotonicity\", \"performance rmse\"]]\n",
    "df_clean[\"size\"] = df_clean[\"size\"].astype(str) + \" B\"\n",
    "\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_clean.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check overlap in in results (to know what should be boldface in the table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to your DataFrame for monotonicity\n",
    "# df_sorted = df_metric.sort_values(by=[\"family\", \"size\", \"context\"], ascending=True)\n",
    "df_sorted = df_metric.sort_values(by=[\"mean_monotonicity\"], ascending=False)\n",
    "df_with_overlap = check_overlap(df_sorted, 'mean_monotonicity', 'stderr_monotonicity')\n",
    "\n",
    "# Show relevant columns\n",
    "df_with_overlap[['family', 'size', 'context', 'mean_monotonicity', 'stderr_monotonicity', \n",
    "                'lower_bound', 'upper_bound', 'overlap_with_prev', 'overlap_with_next']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to your DataFrame for monotonicity\n",
    "# df_sorted = df_metric.sort_values(by=[\"family\", \"size\", \"context\"], ascending=True)\n",
    "df_sorted = df_metric.sort_values(by=[\"mean_rmse\"], ascending=True)\n",
    "df_with_overlap = check_overlap(df_sorted, 'mean_rmse', 'stderr_rmse')\n",
    "\n",
    "# Show relevant columns\n",
    "df_with_overlap[['family', 'size', 'context', 'mean_rmse', 'stderr_rmse', \n",
    "                'lower_bound', 'upper_bound', 'overlap_with_prev', 'overlap_with_next']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional: LLM question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for config_id in config_ids:\n",
    "#     logger.info(f\"Plotting student level performance\", config_id=config_id)\n",
    "#     plot_student_level_performance(\n",
    "#         exp_name=EXP_NAME,\n",
    "#         config_id=config_id,\n",
    "#         metric=\"val_accuracy\",\n",
    "#         **legend_kwargs,\n",
    "#         save=False,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tools.utils import ensure_dir\n",
    "\n",
    "\n",
    "def _plot_level_correctness_roleplay(\n",
    "    df_results: pd.DataFrame,\n",
    "    config_id: str = None,\n",
    "    save: bool = False,\n",
    "    savefig_kwargs: Optional[dict] = None,\n",
    "):\n",
    "    \"\"\"Plot LLM correctness per level.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_results : pd.DataFrame\n",
    "        DataFrame containing the results to plot.\n",
    "    config_id : str, optional\n",
    "        Configuration ID, by default None\n",
    "    save : bool, optional\n",
    "        Whether to save the plot, by default False\n",
    "    savefig_kwargs : Optional[dict], optional\n",
    "        Dictionary with save arguments, by default None\n",
    "    \"\"\"\n",
    "    llm_group_correctness = (\n",
    "        df_results[df_results[\"config_id\"] == config_ids[0]]\n",
    "        .filter(regex=(\".*_llm_group_correctness\"))\n",
    "        .iloc[0, 0]\n",
    "    )\n",
    "    df_llm = pd.DataFrame({\n",
    "        \"student_level_group\": range(1, len(llm_group_correctness) + 1),\n",
    "        \"llm_correct\": llm_group_correctness\n",
    "    }).set_index(\"student_level_group\")\n",
    "    print(df_llm)\n",
    "\n",
    "    _, ax = plt.subplots()\n",
    "    df_llm.plot(kind=\"line\", ax=ax, label=\"LLM\")  # FIXME: label is not shown\n",
    "    ax.set(\n",
    "        xlabel=\"Student levels\",\n",
    "        ylabel=\"MCQ correctness\",\n",
    "    )\n",
    "    ax.set_ylim(-0.05, 1.05)\n",
    "    ax.set_title((None if save else config_id), fontsize=9)\n",
    "    ax.legend(loc=\"upper left\", fontsize=9)\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "    if save:\n",
    "        plt.tight_layout()\n",
    "        ensure_dir(os.path.dirname(savefig_kwargs[\"fname\"]))\n",
    "        plt.savefig(**savefig_kwargs)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # all configs\n",
    "# for config_id in config_ids:\n",
    "#     _plot_level_correctness_roleplay(\n",
    "#         df_results=df_results,\n",
    "#         config_id=config_id,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distractor alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: obtain answer proportion for all questions in valsmall set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# df_interactions = pd.read_csv(\"../data/silver/dbe_kt22_interactions.csv\")\n",
    "# df_q_val = pd.read_csv(\"../data/gold/dbe_kt22_questions_validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_i_val = df_interactions[df_interactions[\"question_id\"].isin(df_q_val[\"question_id\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get proportions as a DataFrame with options as columns\n",
    "# prop_df = (df_i_val.groupby([\"question_id\", \"student_level_group\"])[\"student_option_id\"]\n",
    "#            .value_counts(normalize=True)\n",
    "#            .unstack(fill_value=0.0)\n",
    "#            .reset_index())\n",
    "# # convert 4 columns to dict\n",
    "# prop_df[\"dict\"] = prop_df.set_index(['question_id', 'student_level_group']).to_dict('index').values()\n",
    "# prop_df = prop_df.drop(columns=[1, 2, 3, 4])\n",
    "# prop_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_dict = get_llm_student_preds(\n",
    "#         exp_name=EXP_NAME,\n",
    "#         config_id=\"qwen3:8b~T_0.0~SO_student_bool~L_5~SP_student_chocolate_level_nocontext~SS_proficiency_5_str~EFQ_quotes~EFI_quotes~ES_miscon_studentlevel_random0\",\n",
    "#         run_id=1,\n",
    "#         split=\"val\",\n",
    "#         problem_type=PROBLEM_TYPE,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "\n",
    "# def alignment_score_single(\n",
    "#     y_true: int, y_llm: int, dict_props: dict\n",
    "# ) -> float:\n",
    "#     \"\"\"Calculate the alignment score for a single question.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     y_true : int\n",
    "#         The true answer.\n",
    "#     y_llm : int\n",
    "#         The LLM predicted answer.\n",
    "#     dict_props : dict\n",
    "#         A dictionary mapping answer options to student proportions.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     float\n",
    "#         The alignment score.\n",
    "#     \"\"\"  \n",
    "#     llm_answer_incorrect = y_true != y_llm\n",
    "#     if llm_answer_incorrect:\n",
    "#         dict_tmp = dict_props.copy()\n",
    "#         prop_answer_llm = dict_tmp[y_llm]\n",
    "#         # remove correct idx from dict\n",
    "#         dict_tmp.pop(y_true, None)\n",
    "#         idx_most_popular_distractor = max(dict_tmp, key=dict_tmp.get)\n",
    "#         prop_most_popular_distractor = dict_tmp[idx_most_popular_distractor]\n",
    "#         # calculate score\n",
    "#         try:\n",
    "#             score = prop_answer_llm / prop_most_popular_distractor\n",
    "#         except ZeroDivisionError:\n",
    "#             score = 0.0\n",
    "#     else:\n",
    "#         score = np.nan\n",
    "#     return score\n",
    "\n",
    "# alignment_score_single(\n",
    "#     y_true=3,\n",
    "#     y_llm=4,\n",
    "#     dict_props={1: 0.07692307692307693, 2: 0.2692307692307692, 3: 0.6538461538461539, 4: 0.0}\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from operator import itemgetter\n",
    "# import numpy as np\n",
    "# from numpy.typing import NDArray\n",
    "# import pandas as pd\n",
    "\n",
    "\n",
    "# def eval_distractor_alignment(\n",
    "#     y_true_array: NDArray,\n",
    "#     y_llm_array: NDArray,\n",
    "#     student_level_group_array: NDArray,\n",
    "#     question_id_array: NDArray,\n",
    "#     student_scale_map: dict,\n",
    "#     prop_df: pd.DataFrame,\n",
    "# ) -> float:\n",
    "#     \"\"\"Evaluate the alignment of distractor answers.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     y_true_array : NDArray\n",
    "#         The true answers.\n",
    "#     y_llm_array : NDArray\n",
    "#         The LLM predicted answers.\n",
    "#     student_level_group_array : NDArray\n",
    "#         The student level groups.\n",
    "#     question_id_array : NDArray\n",
    "#         The question IDs.\n",
    "#     student_scale_map : dict\n",
    "#         A mapping from student IDs to their scale.\n",
    "#     prop_df : pd.DataFrame\n",
    "#         A DataFrame containing the student proportions of each answer option.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     float\n",
    "#         The mean alignment score.\n",
    "#     \"\"\"\n",
    "#     dict_inverse = {v: int(k) for k, v in student_scale_map.items()}\n",
    "#     student_level_group_array_int = np.array(\n",
    "#         itemgetter(*student_level_group_array)(dict_inverse)\n",
    "#     )\n",
    "#     print(student_level_group_array_int)\n",
    "\n",
    "#     scores = []\n",
    "#     for y_true, y_llm, student_level_group, question_id in zip(\n",
    "#         y_true_array, y_llm_array, student_level_group_array_int, question_id_array\n",
    "#     ):\n",
    "#         dict_tmp = (\n",
    "#             prop_df[\n",
    "#                 (prop_df[\"question_id\"] == question_id)\n",
    "#                 & (prop_df[\"student_level_group\"] == student_level_group)\n",
    "#             ][\"dict\"]\n",
    "#             .item()\n",
    "#             .copy()\n",
    "#         )\n",
    "#         score = alignment_score_single(y_true=y_true, y_llm=y_llm, dict_props=dict_tmp)\n",
    "#         scores.append(score)\n",
    "\n",
    "#     # compute mean and ignore NaNs\n",
    "#     mean_score = np.nanmean(scores) if scores else 0.0\n",
    "#     return mean_score\n",
    "\n",
    "\n",
    "# eval_distractor_alignment(\n",
    "#     y_true_array=preds_dict[\"y_true\"],\n",
    "#     y_llm_array=preds_dict[\"y_pred\"],\n",
    "#     student_level_group_array=preds_dict[\"student_level_group\"],\n",
    "#     question_id_array=preds_dict[\"question_ids\"],\n",
    "#     student_scale_map=preds_dict[\"student_scale_map\"],\n",
    "#     prop_df=prop_df,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_prop = pd.read_csv(\"../data/platinum/dbe_kt22_proportions_val.csv\")\n",
    "# df_prop[\"dict\"] = df_prop[\"dict\"].apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
