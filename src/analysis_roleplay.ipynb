{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library imports\n",
    "# /\n",
    "\n",
    "# related third party imports\n",
    "import structlog\n",
    "\n",
    "# local application/library specific imports\n",
    "from tools.configurator import (\n",
    "    get_configs_out,\n",
    "    get_config_ids,\n",
    ")\n",
    "from tools.analyzer import (\n",
    "    print_table_from_dict,\n",
    "    print_df_from_dict,\n",
    "    get_results_dict,\n",
    "    merge_all_results,\n",
    "    create_config_id_print,\n",
    "    get_config_df,\n",
    "    check_overlap,\n",
    ")\n",
    "from tools.plotter import (\n",
    "    activate_latex,\n",
    "    deactivate_latex,\n",
    ")\n",
    "\n",
    "\n",
    "logger = structlog.get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### INPUTS #####\n",
    "EXP_NAME = (\n",
    "    # context\n",
    "    # \"roleplay_miscon_test_kt_20250820-204306\"\n",
    "\n",
    "    # no context\n",
    "    # \"roleplay_miscon_test_kt_nocontext_20250821-075331\"\n",
    "\n",
    "    # merged\n",
    "    # \"roleplay_miscon_test_kt_merged_20250821\"\n",
    "\n",
    "    # DBE-KT22\n",
    "    # \"roleplay_dbekt22_val_20250922-194750\"\n",
    "    \"roleplay_dbekt22_test_20250924-090854\"\n",
    "\n",
    "    # CFE\n",
    "    # \"roleplay_cupacfe_val_20250919-112843\"\n",
    "    # \"roleplay_cupacfe_val_20250925-080936\"    \n",
    "    # \"roleplay_cupacfe_test_20250925-142305\"\n",
    ")\n",
    "SPLIT = \"val\" if \"val\" in EXP_NAME else \"test\"\n",
    "EXCLUDE_METRICS = [\n",
    "    \"val_acc_true_pred\",\n",
    "    \"val_f1_true_pred\",\n",
    "]\n",
    "LEGEND_EXACT = True\n",
    "PROBLEM_TYPE = \"roleplay\"\n",
    "SANS_SERIF = True\n",
    "PRINT_PAPER = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC2LEGEND_DICT = {\n",
    "    f\"{SPLIT}_rmse\": f\"{SPLIT} RMSE\",\n",
    "    f\"{SPLIT}_mae\": f\"{SPLIT} MAE\",\n",
    "    f\"{SPLIT}_llm_correctness\": \"llm correctness\",\n",
    "    f\"{SPLIT}_monotonicity\": f\"{SPLIT} monotonicity\",\n",
    "    f\"{SPLIT}_prop_invalid\": f\"{SPLIT} prop invalid\",\n",
    "    f\"{SPLIT}_distractor_alignment\": f\"{SPLIT} distr alignment\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = get_configs_out(EXP_NAME)\n",
    "config_ids = get_config_ids(configs, problem_type=PROBLEM_TYPE)\n",
    "config_dict = {config_id: cfg for config_id, cfg in zip(config_ids, configs)}\n",
    "\n",
    "CONFIG2LEGEND_DICT = {\n",
    "    config_id: create_config_id_print(config_id) for config_id in config_ids\n",
    "}\n",
    "legend_kwargs = {\n",
    "    \"config2legend\": CONFIG2LEGEND_DICT,\n",
    "    \"legend_exact\": LEGEND_EXACT,\n",
    "    \"metric2legend\": METRIC2LEGEND_DICT,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge results for all configs\n",
    "run_id_dict = merge_all_results(EXP_NAME, config_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Val/Test set performance\n",
    "## Complete table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = get_results_dict(\n",
    "    exp_name=EXP_NAME,\n",
    "    config_ids=config_ids,\n",
    "    run_id=None,\n",
    ")\n",
    "# # NOTE: print paper-like table with this code\n",
    "# print_table_from_dict(\n",
    "#     eval_dict=results_dict,\n",
    "#     exp_name=EXP_NAME,\n",
    "#     exclude_metrics=EXCLUDE_METRICS,\n",
    "#     decimals=3,\n",
    "#     **legend_kwargs,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: print dataframe\n",
    "df = print_df_from_dict(\n",
    "    eval_dict=results_dict,\n",
    "    exp_name=EXP_NAME,\n",
    "    exclude_metrics=EXCLUDE_METRICS,\n",
    "    **legend_kwargs,\n",
    "    # save=True,\n",
    "    # save_kwargs={\"fname\": os.path.join(\"output\", EXP_NAME, \"results.csv\")},\n",
    ")\n",
    "\n",
    "df_config = get_config_df(config_dict)\n",
    "\n",
    "# mean\n",
    "df_mean = df.xs('mean', axis=1, level=1, drop_level=True)\n",
    "df_results = df_mean.merge(df_config, how=\"left\", on=\"config_id\")\n",
    "df_results = df_results.reindex(\n",
    "    columns=(\n",
    "        list(df_config.columns)\n",
    "        + list([a for a in df_mean.columns if a not in df_config.columns])\n",
    "    )\n",
    ")\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard error\n",
    "df_stderr = df.xs(\"stderr\", axis=1, level=1, drop_level=True)\n",
    "df_stderr = df_stderr.merge(df_config, how=\"left\", on=\"config_id\")\n",
    "df_stderr = df_stderr.reindex(\n",
    "    columns=(\n",
    "        list(df_config.columns)\n",
    "        + list([a for a in df_stderr.columns if a not in df_config.columns])\n",
    "    )\n",
    ")\n",
    "# df_stderr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating results over characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"kt\" in EXP_NAME:\n",
    "    agg_dict = {\n",
    "        f\"{SPLIT} RMSE\": \"mean\",\n",
    "        f\"{SPLIT} MAE\": \"mean\",\n",
    "        f\"{SPLIT} monotonicity\": \"mean\",\n",
    "        \"llm correctness\": \"mean\",\n",
    "    }\n",
    "else:\n",
    "    agg_dict = {\n",
    "        f\"{SPLIT} RMSE\": \"mean\",\n",
    "        f\"{SPLIT} MAE\": \"mean\",\n",
    "        f\"{SPLIT} monotonicity\": \"mean\",\n",
    "        # f\"{SPLIT} distr alignment\": \"mean\",\n",
    "        \"llm correctness\": \"mean\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect average performance per config value\n",
    "FEATURE = \"num_examples\"\n",
    "df_results.groupby(FEATURE).agg(agg_dict).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contextual models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_context = df_results[df_results[\"num_examples\"] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect average performance per config value\n",
    "FEATURE = \"context\"\n",
    "df_results_context.groupby(FEATURE).agg(agg_dict).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect average performance per config value\n",
    "FEATURE = \"num_examples\"\n",
    "df_results_context.groupby(FEATURE).agg(agg_dict).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect average performance per config value\n",
    "FEATURE = \"model\"\n",
    "df_results_context.groupby(FEATURE).agg(agg_dict).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect average performance per config value\n",
    "FEATURE = \"temp\"\n",
    "df_results_context.groupby(FEATURE).agg(agg_dict).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect average performance per config value\n",
    "FEATURE = \"prompt\"\n",
    "df_results_context.groupby(FEATURE).agg(agg_dict).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect average performance per config value\n",
    "FEATURE = \"example_selec\"\n",
    "df_results_context.groupby(FEATURE).agg(agg_dict).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect average performance per config value\n",
    "FEATURE = \"struc_output\"\n",
    "df_results_context.groupby(FEATURE).agg(agg_dict).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the full rows for these best-performing configs\n",
    "metric = f\"{SPLIT} RMSE\"\n",
    "\n",
    "best_indices = df_results_context.groupby([\"model\"])[metric].idxmin()  # NOTE: min because RMSE\n",
    "best_configs = df_results_context.loc[best_indices]\n",
    "best_configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-contextual models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_nocontext = df_results[df_results[\"num_examples\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_nocontext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_results_nocontext.empty:\n",
    "    # inspect average performance per config value\n",
    "    FEATURE = \"context\"\n",
    "    display(df_results_nocontext.groupby(FEATURE).agg(agg_dict).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_results_nocontext.empty:\n",
    "    # inspect average performance per config value\n",
    "    FEATURE = \"num_examples\"\n",
    "    display(df_results_nocontext.groupby(FEATURE).agg(agg_dict).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_results_nocontext.empty:\n",
    "    # inspect average performance per config value\n",
    "    FEATURE = \"model\"\n",
    "    display(df_results_nocontext.groupby(FEATURE).agg(agg_dict).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_results_nocontext.empty:\n",
    "    # inspect average performance per config value\n",
    "    FEATURE = \"temp\"\n",
    "    display(df_results_nocontext.groupby(FEATURE).agg(agg_dict).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_results_nocontext.empty:\n",
    "    # inspect average performance per config value\n",
    "    FEATURE = \"prompt\"\n",
    "    display(df_results_nocontext.groupby(FEATURE).agg(agg_dict).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_results_nocontext.empty:\n",
    "    # inspect average performance per config value\n",
    "    FEATURE = \"example_selec\"\n",
    "    display(df_results_nocontext.groupby(FEATURE).agg(agg_dict).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_results_nocontext.empty:\n",
    "    # inspect average performance per config value\n",
    "    FEATURE = \"struc_output\"\n",
    "    display(df_results_nocontext.groupby(FEATURE).agg(agg_dict).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the full rows for these best-performing configs\n",
    "metric = f\"{SPLIT} RMSE\"\n",
    "\n",
    "best_indices = df_results_nocontext.groupby([\"model\"])[metric].idxmin()  # NOTE: min because RMSE\n",
    "best_configs = df_results_nocontext.loc[best_indices]\n",
    "best_configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual & Non-contextual models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metric = df[[f\"{SPLIT} monotonicity\", f\"{SPLIT} RMSE\"]].droplevel(0, axis=1)\n",
    "df_metric.columns = ['mean_monotonicity', 'stderr_monotonicity', \"mean_rmse\", \"stderr_rmse\"]\n",
    "df_metric = df_metric.merge(df_config, how=\"left\", on=\"config_id\")\n",
    "df_metric = df_metric.reindex(\n",
    "    columns=(\n",
    "        list(df_config.columns)\n",
    "        + list([a for a in df_metric.columns if a not in df_config.columns])\n",
    "    )\n",
    ")\n",
    "df_metric[\"performance monotonicity\"] = df_metric.apply(lambda x: f\"{x['mean_monotonicity']:.3f} \\gray{{$\\pm$ {x['stderr_monotonicity']:.3f}}}\", axis=1)\n",
    "df_metric[\"performance rmse\"] = df_metric.apply(lambda x: f\"{x['mean_rmse']:.3f} \\gray{{$\\pm$ {x['stderr_rmse']:.3f}}}\", axis=1)\n",
    "# extract model family and size\n",
    "df_metric[\"family\"] = df_metric[\"model\"].str.extract(r\"^(.*?):\")[0]\n",
    "df_metric[\"size\"] = (\n",
    "        df_metric[\"model\"].str.extract(r\":(\\d+\\.?\\d*)b$\")[0].astype(float).round(1)\n",
    "    )\n",
    "# create new column to group on having context or not\n",
    "context_map = {True: \"Context\", False: \"No context\"}\n",
    "df_metric[\"context\"] = df_metric[\"prompt\"].str.contains(\"_context\").map(context_map)\n",
    "# clean prompt persona\n",
    "prompt_map = {\"teacher_kt\": \"Teacher\", \"student_kt\": \"Student\", \"student_bool_nocontext\": \"Student\", \"teacher_bool_nocontext\": \"Teacher\"}\n",
    "df_metric[\"persona\"] = df_metric[\"struc_output\"].map(prompt_map)\n",
    "# clean example selector\n",
    "example_selec_map = {\n",
    "    \"miscon_studentid_kc_exact\": \"Knowledge Concept\",\n",
    "    \"miscon_studentid_random\": \"Random\",\n",
    "    \"both_dbe_studentid_random\": \"Random\",\n",
    "    \"both_dbe_studentid_kc_exact\": \"Knowledge Concept\",\n",
    "    \"both_cfe_studentid_random\": \"Random\",\n",
    "    \"both_cfe_studentlevel_random\": \"Random\",\n",
    "    \"both_dbe_studentlevel_random\": \"Random\",\n",
    "    \"both_dbe_studentlevel_kc_exact\": \"Knowledge Concept\",\n",
    "}\n",
    "df_metric[\"example_selec\"] = df_metric[\"example_selec\"].map(example_selec_map)\n",
    "# round temp\n",
    "df_metric[\"temp\"] = df_metric[\"temp\"].round(1).astype(str)\n",
    "# insert \"\\NA\" for example selector if no context\n",
    "df_metric.loc[df_metric[\"context\"] == \"No context\", \"example_selec\"] = \"\\\\NA\"\n",
    "\n",
    "df_metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [\"family\", \"size\", \"context\", \"performance rmse\", \"performance monotonicity\", \"persona\", \"example_selec\", \"num_examples\", \"temp\"]\n",
    "\n",
    "df_clean = df_metric.sort_values(by=[\"family\", \"size\", \"context\"], ascending=True)[col_names]\n",
    "df_clean[\"size\"] = df_clean[\"size\"].astype(str) + \" B\"\n",
    "\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_clean.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check overlap in results (to know what should be boldface in the table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to your DataFrame for monotonicity\n",
    "# df_sorted = df_metric.sort_values(by=[\"family\", \"size\", \"context\"], ascending=True)\n",
    "df_sorted = df_metric.sort_values(by=[\"mean_monotonicity\"], ascending=False)\n",
    "df_with_overlap = check_overlap(df_sorted, 'mean_monotonicity', 'stderr_monotonicity')\n",
    "\n",
    "# Show relevant columns\n",
    "df_with_overlap[['family', 'size', 'context', 'mean_monotonicity', 'stderr_monotonicity', \n",
    "                'lower_bound', 'upper_bound', 'overlap_with_prev', 'overlap_with_next']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to your DataFrame for monotonicity\n",
    "df_sorted = df_metric.sort_values(by=[\"family\", \"size\", \"context\"], ascending=True)\n",
    "# df_sorted = df_metric.sort_values(by=[\"mean_rmse\"], ascending=True)\n",
    "df_with_overlap = check_overlap(df_sorted, 'mean_rmse', 'stderr_rmse')\n",
    "\n",
    "# Show relevant columns\n",
    "df_with_overlap[['family', 'size', 'context', 'mean_rmse', 'stderr_rmse', \n",
    "                'lower_bound', 'upper_bound', 'overlap_with_prev', 'overlap_with_next']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_q_test = pd.read_csv(\"../data/gold/dbe_kt22_questions_test.csv\")\n",
    "\n",
    "df_i_silver = pd.read_csv(\"../data/silver/dbe_kt22_interactions.csv\")\n",
    "df_i_test = df_i_silver[df_i_silver[\"question_id\"].isin(df_q_test[\"question_id\"].unique())]\n",
    "# df_ctt = df_i_test.groupby(\"question_id\").agg({\"student_option_correct\": [\"mean\"]})\n",
    "df_ctt = df_i_test.groupby(\"question_id\")[[\"student_option_correct\"]].mean()\n",
    "df_ctt[\"ctt_difficulty\"] = 1 - df_ctt[\"student_option_correct\"]\n",
    "df_ctt = df_ctt.drop(columns=[\"student_option_correct\"]).reset_index()\n",
    "df_ctt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.analyzer import read_pickle\n",
    "from typing import Any\n",
    "import os\n",
    "\n",
    "def get_irt_df(experiment: str, config_id: str, run_id: int) -> dict:\n",
    "    # paths\n",
    "    output_dir = os.path.join(\"output\", experiment)\n",
    "    output_path = os.path.join(output_dir, config_id, f\"run_{run_id}.pickle\")\n",
    "\n",
    "    logger.info(\"Loading checkpoint\", output_path=output_path)\n",
    "    log_dict = read_pickle(output_path)\n",
    "    irt_df = log_dict[\"preds_qdiff\"][\"test_df_input\"]\n",
    "    y_true = log_dict[\"preds_qdiff\"][\"test_y_true\"]\n",
    "    return irt_df, y_true\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "irt_df, y_true = get_irt_df(\n",
    "    experiment=EXP_NAME,\n",
    "    config_id=\"llama3.2:1b~T_0.0~SO_student_bool_nocontext~L_5~SP_student_dbe_miscons_level_context~SS_proficiency_5_str~EFQ_quotes~EFI_quotes~ES_both_dbe_studentlevel_random1\",\n",
    "    run_id=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_ctt = irt_df.copy()\n",
    "\n",
    "df_pred_ctt = irt_df.groupby(\"question_id\")[[\"student_option_correct\"]].mean()\n",
    "df_pred_ctt[\"ctt_difficulty\"] = 1 - df_pred_ctt[\"student_option_correct\"]\n",
    "df_pred_ctt = df_pred_ctt.drop(columns=[\"student_option_correct\"]).reset_index()\n",
    "df_pred_ctt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "res = stats.linregress(x=df_q_test[\"q_difficulty\"], y=df_q_test[\"q_difficulty_pred\"])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "# ax.plot([-5, -5], [5, 5], linestyle='--', color='gray')\n",
    "ax.axline((1, 1), slope=1)\n",
    "ax.set_xlabel('True Difficulty', fontsize=14)\n",
    "ax.set_ylabel('Predicted Difficulty', fontsize=14)\n",
    "# ax.set_xlim(-5, 5)\n",
    "# ax.set_title('Predicted vs True Difficulty', fontsize=16)\n",
    "ax.scatter(x=df_ctt[\"ctt_difficulty\"], y=df_pred_ctt[\"ctt_difficulty\"])\n",
    "ax.plot(df_q_test[\"q_difficulty\"], res.intercept + res.slope*df_q_test[\"q_difficulty\"], 'r', label='fitted line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine LLMs for IRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "family_models = df_metric[(df_metric[\"family\"] == \"qwen3\") & (df_metric[\"context\"] == \"Context\")][\"config_id\"].values.tolist()\n",
    "family_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict[family_models[0]][\"LOADER\"][\"NAME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from tools.irt_estimator import irt_estimation\n",
    "from tools.analyzer import read_pickle\n",
    "\n",
    "\n",
    "def get_irt_df(experiment: str, config_id: str, run_id: int) -> dict:\n",
    "    # paths\n",
    "    output_dir = os.path.join(\"output\", experiment)\n",
    "    output_path = os.path.join(output_dir, config_id, f\"run_{run_id}.pickle\")\n",
    "\n",
    "    logger.info(\"Loading checkpoint\", output_path=output_path)\n",
    "    log_dict = read_pickle(output_path)\n",
    "    irt_df = log_dict[\"preds_qdiff\"][\"test_df_input\"]\n",
    "    y_true = log_dict[\"preds_qdiff\"][\"test_y_true\"]\n",
    "    return irt_df, y_true\n",
    "\n",
    "\n",
    "def hybrid_multi_roleplaying_llm(experiment: str, dataset_name: str, config_ids: list[str], run_id: int):\n",
    "    # obtain one large Dataframe for IRT\n",
    "    irt_df_all = pd.DataFrame()\n",
    "    for i, config_id in enumerate(config_ids):\n",
    "        irt_df, _ = get_irt_df(\n",
    "            experiment=experiment,\n",
    "            config_id=config_id,\n",
    "            run_id=run_id,\n",
    "        )\n",
    "        # append i to student_id in irt_df to make unique\n",
    "        irt_df[\"student_level\"] = irt_df[\"student_id\"]\n",
    "        irt_df[\"student_id\"] = irt_df[\"student_id\"] + f\"_model{i}\"\n",
    "        # concatenate\n",
    "        irt_df_all = pd.concat([irt_df_all, irt_df], axis=0)\n",
    "\n",
    "    # Compute IRT parameters\n",
    "    _, difficulty_dict, _ = irt_estimation(interactions_df=irt_df_all)\n",
    "\n",
    "    irt_scale = {\n",
    "        \"dbe_kt22\": (-5,5),\n",
    "        \"cupacfe\": (30, 110),\n",
    "    }\n",
    "    old_range = (-5, 5)\n",
    "\n",
    "    logger.info(\n",
    "        \"Rescaling difficulty range\",\n",
    "        old_range=old_range,\n",
    "        new_range=irt_scale[dataset_name],\n",
    "    )\n",
    "    new_min_diff, new_max_diff = irt_scale[dataset_name]\n",
    "    for key, value in difficulty_dict.items():\n",
    "        difficulty_dict[key] = (\n",
    "            (value - old_range[0]) / (old_range[1] - old_range[0])\n",
    "        ) * (new_max_diff - new_min_diff) + new_min_diff\n",
    "\n",
    "    df_q_test = pd.read_csv(f\"../data/gold/{dataset_name}_questions_test.csv\")\n",
    "    df_q_test = df_q_test[[\"question_id\", \"q_difficulty\"]]\n",
    "\n",
    "    df_q_test[\"q_difficulty_pred\"] = df_q_test[\"question_id\"].map(difficulty_dict)\n",
    "\n",
    "    rmse = root_mean_squared_error(y_true=df_q_test[\"q_difficulty\"], y_pred=df_q_test[\"q_difficulty_pred\"])\n",
    "    return rmse\n",
    "\n",
    "# dataset_name = config_dict[family_models[0]][\"LOADER\"][\"NAME\"]\n",
    "# hybrid_multi_roleplaying_llm(\n",
    "#     experiment=EXP_NAME,\n",
    "#     dataset_name=dataset_name,\n",
    "#     config_ids=family_models,\n",
    "#     run_id=1,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.analyzer import mean_stderror\n",
    "\n",
    "def hybrid_multi_roleplaying_llm_all_runs(experiment: str, dataset_name: str, config_ids: list[str]):\n",
    "    rmse_scores = []\n",
    "    for run_id in [1, 2, 3]:\n",
    "        rmse = hybrid_multi_roleplaying_llm(\n",
    "            experiment=experiment,\n",
    "            dataset_name=dataset_name,\n",
    "            config_ids=config_ids,\n",
    "            run_id=run_id,\n",
    "        )\n",
    "        rmse_scores.append(rmse)\n",
    "    \n",
    "    mean_rmse, stderr_rmse = mean_stderror(rmse_scores)\n",
    "    print()\n",
    "    print(f\"Hybrid multi-roleplaying LLM RMSE: {mean_rmse:.3f} Â± {stderr_rmse:.3f}  ({rmse_scores})\")\n",
    "    return rmse_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "family_models = df_metric[(df_metric[\"family\"] == \"qwen3\") & (df_metric[\"context\"] == \"Context\")][\"config_id\"].values.tolist()\n",
    "dataset_name = config_dict[family_models[0]][\"LOADER\"][\"NAME\"]\n",
    "rmse_scores = hybrid_multi_roleplaying_llm_all_runs(\n",
    "    experiment=EXP_NAME,\n",
    "    dataset_name=dataset_name,\n",
    "    config_ids=family_models,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "family_models = df_metric[(df_metric[\"family\"] == \"qwen3\") & (df_metric[\"context\"] == \"No context\")][\"config_id\"].values.tolist()\n",
    "dataset_name = config_dict[family_models[0]][\"LOADER\"][\"NAME\"]\n",
    "rmse_scores = hybrid_multi_roleplaying_llm_all_runs(\n",
    "    experiment=EXP_NAME,\n",
    "    dataset_name=dataset_name,\n",
    "    config_ids=family_models,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "family_models = df_metric[(df_metric[\"family\"].isin([\"llama3.1\", \"llama3.2\"])) & (df_metric[\"context\"] == \"Context\")][\"config_id\"].values.tolist()\n",
    "dataset_name = config_dict[family_models[0]][\"LOADER\"][\"NAME\"]\n",
    "rmse_scores = hybrid_multi_roleplaying_llm_all_runs(\n",
    "    experiment=EXP_NAME,\n",
    "    dataset_name=dataset_name,\n",
    "    config_ids=family_models,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "family_models = df_metric[(df_metric[\"family\"].isin([\"llama3.1\", \"llama3.2\"])) & (df_metric[\"context\"] == \"No context\")][\"config_id\"].values.tolist()\n",
    "dataset_name = config_dict[family_models[0]][\"LOADER\"][\"NAME\"]\n",
    "rmse_scores = hybrid_multi_roleplaying_llm_all_runs(\n",
    "    experiment=EXP_NAME,\n",
    "    dataset_name=dataset_name,\n",
    "    config_ids=family_models,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one large Dataframe for IRT\n",
    "irt_df_all = pd.DataFrame()\n",
    "family_models = df_metric[(df_metric[\"family\"] == \"qwen3\") & (df_metric[\"context\"] == \"Context\")][\"config_id\"].values.tolist()\n",
    "\n",
    "\n",
    "for i, config_id in enumerate(family_models):\n",
    "    irt_df, y_true = get_irt_df(\n",
    "        experiment=EXP_NAME,\n",
    "        config_id=config_id,\n",
    "        run_id=1,\n",
    "    )\n",
    "    # append i to student_id in irt_df to make unique\n",
    "    irt_df[\"student_id\"] = irt_df[\"student_id\"] + f\"_model{i}\"\n",
    "    # concatenate\n",
    "    irt_df_all = pd.concat([irt_df_all, irt_df], axis=0)\n",
    "\n",
    "irt_df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.irt_estimator import irt_estimation\n",
    "\n",
    "# Compute IRT parameters\n",
    "student_dict, difficulty_dict, _ = irt_estimation(interactions_df=irt_df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irt_scale = {\n",
    "    \"dbe_kt22\": (-5,5),\n",
    "    \"cupacfe\": (30, 110),\n",
    "}\n",
    "old_range = (-5, 5)\n",
    "\n",
    "dataset_name = config_dict[family_models[0]][\"LOADER\"][\"NAME\"]\n",
    "\n",
    "logger.info(\n",
    "    \"Rescaling difficulty range\",\n",
    "    old_range=old_range,\n",
    "    new_range=irt_scale[dataset_name],\n",
    ")\n",
    "new_min_diff, new_max_diff = irt_scale[dataset_name]\n",
    "for key, value in difficulty_dict.items():\n",
    "    difficulty_dict[key] = (\n",
    "        (value - old_range[0]) / (old_range[1] - old_range[0])\n",
    "    ) * (new_max_diff - new_min_diff) + new_min_diff\n",
    "\n",
    "df_q_test = pd.read_csv(f\"../data/gold/{dataset_name}_questions_test.csv\")\n",
    "df_q_test = df_q_test[[\"question_id\", \"q_difficulty\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_q_test[\"q_difficulty_pred\"] = df_q_test[\"question_id\"].map(difficulty_dict)\n",
    "df_q_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "root_mean_squared_error(y_true=df_q_test[\"q_difficulty\"], y_pred=df_q_test[\"q_difficulty_pred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "res = stats.linregress(x=df_q_test[\"q_difficulty\"], y=df_q_test[\"q_difficulty_pred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "# ax.plot([-5, -5], [5, 5], linestyle='--', color='gray')\n",
    "ax.axline((1, 1), slope=1)\n",
    "ax.set_xlabel('True Difficulty', fontsize=14)\n",
    "ax.set_ylabel('Predicted Difficulty', fontsize=14)\n",
    "# ax.set_xlim(-5, 5)\n",
    "# ax.set_title('Predicted vs True Difficulty', fontsize=16)\n",
    "ax.scatter(x=df_q_test[\"q_difficulty\"], y=df_q_test[\"q_difficulty_pred\"])\n",
    "ax.plot(df_q_test[\"q_difficulty\"], res.intercept + res.slope*df_q_test[\"q_difficulty\"], 'r', label='fitted line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tools.irt_estimator import irt_estimation\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_diff_aggregated_irt(\n",
    "    df_metric: pd.DataFrame,\n",
    "    family: str,\n",
    "    context_type: str,\n",
    "    exp_name: str,\n",
    "    config_dict: dict,\n",
    ") -> pd.DataFrame:\n",
    "    # obtain one large Dataframe for IRT\n",
    "    irt_df_all = pd.DataFrame()\n",
    "    if type(family) == str:\n",
    "        family = [family]\n",
    "    family_models = df_metric[\n",
    "        (df_metric[\"family\"].isin(family)) & (df_metric[\"context\"] == context_type)\n",
    "    ][\"config_id\"].values.tolist()\n",
    "\n",
    "    for i, config_id in enumerate(family_models):\n",
    "        irt_df, _ = get_irt_df(\n",
    "            experiment=exp_name,\n",
    "            config_id=config_id,\n",
    "            run_id=1,\n",
    "        )\n",
    "        # append i to student_id in irt_df to make unique\n",
    "        irt_df[\"student_id\"] = irt_df[\"student_id\"] + f\"_model{i}\"\n",
    "        # concatenate\n",
    "        irt_df_all = pd.concat([irt_df_all, irt_df], axis=0)\n",
    "\n",
    "    # Compute IRT parameters\n",
    "    _, difficulty_dict, _ = irt_estimation(interactions_df=irt_df_all)\n",
    "\n",
    "    irt_scale = {\n",
    "        \"dbe_kt22\": (-5, 5),\n",
    "        \"cupacfe\": (30, 110),\n",
    "    }\n",
    "    old_range = (-5, 5)\n",
    "\n",
    "    dataset_name = config_dict[family_models[0]][\"LOADER\"][\"NAME\"]\n",
    "\n",
    "    logger.info(\n",
    "        \"Rescaling difficulty range\",\n",
    "        old_range=old_range,\n",
    "        new_range=irt_scale[dataset_name],\n",
    "    )\n",
    "    new_min_diff, new_max_diff = irt_scale[dataset_name]\n",
    "    for key, value in difficulty_dict.items():\n",
    "        difficulty_dict[key] = (\n",
    "            (value - old_range[0]) / (old_range[1] - old_range[0])\n",
    "        ) * (new_max_diff - new_min_diff) + new_min_diff\n",
    "\n",
    "    df_q_test = pd.read_csv(f\"../data/gold/{dataset_name}_questions_test.csv\")\n",
    "    df_q_test = df_q_test[[\"question_id\", \"q_difficulty\"]]\n",
    "    df_q_test[\"q_difficulty_pred\"] = df_q_test[\"question_id\"].map(difficulty_dict)\n",
    "\n",
    "    return df_q_test\n",
    "\n",
    "\n",
    "def get_diff_aggregated_irt_context_nocontext(\n",
    "    df_metric: pd.DataFrame, family: str, exp_name: str, config_dict: dict\n",
    "):\n",
    "\n",
    "    q_diff_context = get_diff_aggregated_irt(\n",
    "        df_metric, family, \"Context\", exp_name, config_dict\n",
    "    )\n",
    "    q_diff_nocontext = get_diff_aggregated_irt(\n",
    "        df_metric, family, \"No context\", exp_name, config_dict\n",
    "    )\n",
    "\n",
    "    return q_diff_context, q_diff_nocontext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_diff_context, q_diff_nocontext = get_diff_aggregated_irt_context_nocontext(\n",
    "    df_metric=df_metric,\n",
    "    family=\"qwen3\",\n",
    "    exp_name=EXP_NAME,\n",
    "    config_dict=config_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from tools.utils import ensure_dir\n",
    "\n",
    "def plot_aggregated_interactions(\n",
    "    q_diff_context: pd.DataFrame, q_diff_nocontext: pd.DataFrame, savename: Optional[str] = None,\n",
    "):\n",
    "    linreg_context = stats.linregress(\n",
    "        x=q_diff_context[\"q_difficulty\"], y=q_diff_context[\"q_difficulty_pred\"]\n",
    "    )\n",
    "    linreg_nocontext = stats.linregress(\n",
    "        x=q_diff_nocontext[\"q_difficulty\"], y=q_diff_nocontext[\"q_difficulty_pred\"]\n",
    "    )\n",
    "\n",
    "    _, ax = plt.subplots(figsize=(16/3, 9/3))\n",
    "    ax.set_xlabel(\"True Difficulty\", fontsize=12)\n",
    "    ax.set_ylabel(\"Predicted Difficulty\", fontsize=12)\n",
    "\n",
    "    # contextual\n",
    "    ax.scatter(x=q_diff_context[\"q_difficulty\"], y=q_diff_context[\"q_difficulty_pred\"], marker='o')\n",
    "    ax.plot(\n",
    "        q_diff_context[\"q_difficulty\"],\n",
    "        linreg_context.intercept\n",
    "        + linreg_context.slope * q_diff_context[\"q_difficulty\"],\n",
    "        label=f\"Context\",\n",
    "    )\n",
    "    # non-contextual\n",
    "    ax.scatter(\n",
    "        x=q_diff_nocontext[\"q_difficulty\"], y=q_diff_nocontext[\"q_difficulty_pred\"], marker=\"^\"\n",
    "    )\n",
    "    ax.plot(\n",
    "        q_diff_nocontext[\"q_difficulty\"],\n",
    "        linreg_nocontext.intercept\n",
    "        + linreg_nocontext.slope * q_diff_nocontext[\"q_difficulty\"],\n",
    "        label=\"No context\",\n",
    "    )\n",
    "    # diagonal line\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    x = np.linspace(xlim[0], xlim[1], 100)\n",
    "    ax.plot(x, x, linestyle='--', color='gray')\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "\n",
    "    ax.legend()\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "\n",
    "    if savename is not None:\n",
    "        plt.tight_layout()\n",
    "        ensure_dir(os.path.dirname(savename))\n",
    "        plt.savefig(savename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activate_latex(sans_serif=SANS_SERIF)\n",
    "plot_aggregated_interactions(\n",
    "    q_diff_context=q_diff_context,\n",
    "    q_diff_nocontext=q_diff_nocontext,\n",
    ")\n",
    "deactivate_latex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRINT_PAPER = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PRINT_PAPER:\n",
    "    activate_latex(sans_serif=SANS_SERIF)\n",
    "    ########\n",
    "    # plot for every config_id\n",
    "    for llm_family in [\"qwen3\", [\"llama3.1\", \"llama3.2\"]]:\n",
    "\n",
    "        q_diff_context, q_diff_nocontext = get_diff_aggregated_irt_context_nocontext(\n",
    "            df_metric=df_metric,\n",
    "            family=llm_family,\n",
    "            exp_name=EXP_NAME,\n",
    "            config_dict=config_dict,\n",
    "        )\n",
    "        if llm_family == [\"llama3.1\", \"llama3.2\"]:\n",
    "            llm_family = \"llama3\"\n",
    "        fname = os.path.join(\n",
    "            \"output\", EXP_NAME, \"figures\", f\"aggregated_irt_{llm_family}.pdf\"\n",
    "        )\n",
    "        plot_aggregated_interactions(\n",
    "            q_diff_context=q_diff_context,\n",
    "            q_diff_nocontext=q_diff_nocontext,\n",
    "            savename=fname,\n",
    "        )\n",
    "\n",
    "    ########\n",
    "    deactivate_latex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# student_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# student_skills = pd.DataFrame({\"student\": list(student_dict.keys()), \"ability\": list(student_dict.values())})\n",
    "# # remove students \"p_bad\" and \"p_good\"\n",
    "# student_skills = student_skills[~student_skills[\"student\"].isin([\"p_bad\", \"p_good\"])]\n",
    "# # get int before underscore in student column\n",
    "# student_skills[\"student_level\"] = student_skills[\"student\"].str.extract(\"(\\\\d+)\").astype(int)\n",
    "# student_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# student_skills.groupby(\"student_level\").agg({\"ability\": [\"mean\", \"std\", \"count\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(student_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter plot: predicted vs true difficulty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.analyzer import read_pickle\n",
    "from typing import Any\n",
    "import os\n",
    "\n",
    "\n",
    "def get_preds(experiment: str, config_id: str, run_id: int) -> dict:\n",
    "    # paths\n",
    "    output_dir = os.path.join(\"output\", experiment)\n",
    "    output_path = os.path.join(output_dir, config_id, f\"run_{run_id}.pickle\")\n",
    "\n",
    "    # metric\n",
    "    preds = {}\n",
    "\n",
    "    logger.info(\"Loading checkpoint\", output_path=output_path)\n",
    "    log_dict = read_pickle(output_path)\n",
    "    preds[\"y_pred\"] = log_dict[\"preds_qdiff\"][\"test_y_pred\"]\n",
    "    preds[\"y_true\"] = log_dict[\"preds_qdiff\"][\"test_y_true\"]\n",
    "\n",
    "    return preds\n",
    "\n",
    "\n",
    "# preds = get_preds(\n",
    "#     experiment=EXP_NAME,\n",
    "#     config_id=\"llama3.2:1b~T_0.0~SO_student_bool_nocontext~L_5~SP_student_dbe_miscons_level_context~SS_proficiency_5_str~EFQ_quotes~EFI_quotes~ES_both_dbe_studentlevel_random1\",\n",
    "#     run_id=1,\n",
    "# )\n",
    "\n",
    "# preds = get_preds(\n",
    "#     experiment=EXP_NAME,\n",
    "#     config_id=\"qwen3:1.7b~T_0.0~SO_student_bool_nocontext~L_5~SP_student_cfe_miscons_level_nocontext~SS_proficiency_5_str~EFQ_mcq_reading_quotes~EFI_open_reading~ES_both_cfe_studentlevel_random0\",\n",
    "#     run_id=1,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = read_pickle(\"./output/roleplay_dbekt22_test_20250924-090854/qwen3:14b~T_1.0~SO_teacher_bool_nocontext~L_5~SP_teacher_dbe_miscons_level_context~SS_proficiency_5_str~EFQ_quotes~EFI_quotes~ES_both_dbe_studentlevel_kc_exact3/run_1.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "# ax.plot([-5, -5], [5, 5], linestyle='--', color='gray')\n",
    "ax.axline((1, 1), slope=1)\n",
    "ax.set_xlabel('True Difficulty', fontsize=14)\n",
    "ax.set_ylabel('Predicted Difficulty', fontsize=14)\n",
    "# ax.set_xlim(-5, 5)\n",
    "# ax.set_title('Predicted vs True Difficulty', fontsize=16)\n",
    "ax.scatter(x=preds[\"y_true\"], y=preds[\"y_pred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(preds[\"y_true\"], bins=10, alpha=0.5, label='True Difficulty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df_results_nocontext, x=\"test RMSE\", y=\"llm correctness\", hue=\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_nocontext[[\"model\", \"test monotonicity\", \"llm correctness\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.scatterplot(data=df_results_nocontext, x=\"test RMSE\", y=\"llm correctness\", hue=\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional: LLM question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for config_id in config_ids:\n",
    "#     logger.info(f\"Plotting student level performance\", config_id=config_id)\n",
    "#     plot_student_level_performance(\n",
    "#         exp_name=EXP_NAME,\n",
    "#         config_id=config_id,\n",
    "#         metric=\"val_accuracy\",\n",
    "#         **legend_kwargs,\n",
    "#         save=False,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tools.utils import ensure_dir\n",
    "\n",
    "\n",
    "def _plot_level_correctness_roleplay(\n",
    "    df_results: pd.DataFrame,\n",
    "    config_id: str = None,\n",
    "    save: bool = False,\n",
    "    savefig_kwargs: Optional[dict] = None,\n",
    "):\n",
    "    \"\"\"Plot LLM correctness per level.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_results : pd.DataFrame\n",
    "        DataFrame containing the results to plot.\n",
    "    config_id : str, optional\n",
    "        Configuration ID, by default None\n",
    "    save : bool, optional\n",
    "        Whether to save the plot, by default False\n",
    "    savefig_kwargs : Optional[dict], optional\n",
    "        Dictionary with save arguments, by default None\n",
    "    \"\"\"\n",
    "    llm_group_correctness = (\n",
    "        df_results[df_results[\"config_id\"] == config_ids[0]]\n",
    "        .filter(regex=(\".*_llm_group_correctness\"))\n",
    "        .iloc[0, 0]\n",
    "    )\n",
    "    df_llm = pd.DataFrame({\n",
    "        \"student_level_group\": range(1, len(llm_group_correctness) + 1),\n",
    "        \"llm_correct\": llm_group_correctness\n",
    "    }).set_index(\"student_level_group\")\n",
    "    print(df_llm)\n",
    "\n",
    "    _, ax = plt.subplots()\n",
    "    df_llm.plot(kind=\"line\", ax=ax, label=\"LLM\")  # FIXME: label is not shown\n",
    "    ax.set(\n",
    "        xlabel=\"Student levels\",\n",
    "        ylabel=\"MCQ correctness\",\n",
    "    )\n",
    "    ax.set_ylim(-0.05, 1.05)\n",
    "    ax.set_title((None if save else config_id), fontsize=9)\n",
    "    ax.legend(loc=\"upper left\", fontsize=9)\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "    if save:\n",
    "        plt.tight_layout()\n",
    "        ensure_dir(os.path.dirname(savefig_kwargs[\"fname\"]))\n",
    "        plt.savefig(**savefig_kwargs)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # all configs\n",
    "# for config_id in config_ids:\n",
    "#     _plot_level_correctness_roleplay(\n",
    "#         df_results=df_results,\n",
    "#         config_id=config_id,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
