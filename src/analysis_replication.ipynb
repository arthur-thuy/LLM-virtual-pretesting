{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library imports\n",
    "import os\n",
    "\n",
    "# related third party imports\n",
    "import structlog\n",
    "\n",
    "# local application/library specific imports\n",
    "from tools.configurator import (\n",
    "    get_configs_out,\n",
    "    get_config_ids,\n",
    ")\n",
    "from tools.analyzer import (\n",
    "    print_table_from_dict,\n",
    "    print_df_from_dict,\n",
    "    get_results_dict,\n",
    "    merge_all_results,\n",
    "    create_config_id_print,\n",
    "    get_config_df,\n",
    "    get_llm_student_preds,\n",
    ")\n",
    "from tools.plotter import (\n",
    "    plot_llm_student_confusion,\n",
    "    plot_kt_confusion,\n",
    "    plot_level_correctness,\n",
    "    activate_latex,\n",
    "    deactivate_latex,\n",
    "    plot_llm_correctness_by_model,\n",
    ")\n",
    "\n",
    "\n",
    "logger = structlog.get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### INPUTS #####\n",
    "EXP_NAME = (\n",
    "    # \"replication_miscon_valsmall_kt_20250818-153513\"\n",
    "    # \"replication_miscon_vallarge_kt_20250819-115545\"\n",
    "    \"replication_miscon_test_kt_20250819-192630\"\n",
    ")\n",
    "SPLIT = \"val\"\n",
    "EXCLUDE_METRICS = []\n",
    "LEGEND_EXACT = True\n",
    "PROBLEM_TYPE = \"replicate\"\n",
    "SANS_SERIF = True\n",
    "PRINT_PAPER = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC2LEGEND_DICT = {\n",
    "    f\"{SPLIT}_acc\": f\"{SPLIT} acc\",\n",
    "    f\"{SPLIT}_bal_acc\": f\"{SPLIT} bal acc\",\n",
    "    f\"{SPLIT}_student_correctness\": f\"student correctness\",\n",
    "    f\"{SPLIT}_llm_correctness\": f\"llm correctness\",\n",
    "    f\"{SPLIT}_acc_kt\": f\"{SPLIT} acc (KT)\",\n",
    "    f\"{SPLIT}_bal_acc_kt\": f\"{SPLIT} bal acc (KT)\",\n",
    "    f\"{SPLIT}_f1_kt\": f\"{SPLIT} f1 (KT)\",\n",
    "    f\"{SPLIT}_prop_invalid\": f\"{SPLIT} prop invalid\",\n",
    "    f\"{SPLIT}_f1_micro\": f\"{SPLIT} f1 (micro)\",\n",
    "    f\"{SPLIT}_f1_macro\": f\"{SPLIT} f1 (macro)\",\n",
    "    f\"{SPLIT}_f1_weighted\": f\"{SPLIT} f1 (weighted)\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = get_configs_out(EXP_NAME)\n",
    "config_ids = get_config_ids(configs, problem_type=PROBLEM_TYPE)\n",
    "config_dict = {config_id: cfg for config_id, cfg in zip(config_ids, configs)}\n",
    "\n",
    "CONFIG2LEGEND_DICT = {\n",
    "    config_id: create_config_id_print(config_id) for config_id in config_ids\n",
    "}\n",
    "legend_kwargs = {\n",
    "    \"config2legend\": CONFIG2LEGEND_DICT,\n",
    "    \"legend_exact\": LEGEND_EXACT,\n",
    "    \"metric2legend\": METRIC2LEGEND_DICT,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge results for all configs\n",
    "run_id_dict = merge_all_results(EXP_NAME, config_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Val/Test set performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = get_results_dict(\n",
    "    exp_name=EXP_NAME,\n",
    "    config_ids=config_ids,\n",
    "    run_id=None,\n",
    ")\n",
    "# # NOTE: print paper-like table with this code\n",
    "# print_table_from_dict(\n",
    "#     eval_dict=results_dict,\n",
    "#     exp_name=EXP_NAME,\n",
    "#     exclude_metrics=EXCLUDE_METRICS,\n",
    "#     decimals=3,\n",
    "#     **legend_kwargs,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: print dataframe\n",
    "df = print_df_from_dict(\n",
    "    eval_dict=results_dict,\n",
    "    exp_name=EXP_NAME,\n",
    "    exclude_metrics=EXCLUDE_METRICS,\n",
    "    **legend_kwargs,\n",
    "    # save=True,\n",
    "    # save_kwargs={\"fname\": os.path.join(\"output\", EXP_NAME, \"results.csv\")},\n",
    ")\n",
    "\n",
    "df_config = get_config_df(config_dict)\n",
    "\n",
    "# mean\n",
    "df_mean = df.xs(\"mean\", axis=1, level=1, drop_level=True)\n",
    "df_results = df_mean.merge(df_config, how=\"left\", on=\"config_id\")\n",
    "df_results = df_results.reindex(\n",
    "    columns=(\n",
    "        list(df_config.columns)\n",
    "        + list([a for a in df_mean.columns if a not in df_config.columns])\n",
    "    )\n",
    ")\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard error\n",
    "df_stderr = df.xs(\"stderr\", axis=1, level=1, drop_level=True)\n",
    "df_stderr = df_stderr.merge(df_config, how=\"left\", on=\"config_id\")\n",
    "df_stderr = df_stderr.reindex(\n",
    "    columns=(\n",
    "        list(df_config.columns)\n",
    "        + list([a for a in df_stderr.columns if a not in df_config.columns])\n",
    "    )\n",
    ")\n",
    "df_stderr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_context = df_results[df_results[\"num_examples\"] > 0]\n",
    "if \"kt\" in EXP_NAME:\n",
    "    agg_dict = {f\"{SPLIT} bal acc (KT)\": \"mean\", f\"{SPLIT} f1 (KT)\": \"mean\", \"llm correctness\": \"mean\"}\n",
    "else:\n",
    "    agg_dict = {f\"{SPLIT} f1 (macro)\": \"mean\", f\"{SPLIT} f1 (KT)\": \"mean\", \"llm correctness\": \"mean\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect average performance per config value\n",
    "FEATURE = \"num_examples\"\n",
    "# TODO: only look at contextual models\n",
    "df_results_context.groupby(FEATURE).agg(agg_dict).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect average performance per config value\n",
    "FEATURE = \"model\"\n",
    "df_results_context.groupby(FEATURE).agg(agg_dict).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect average performance per config value\n",
    "FEATURE = \"temp\"\n",
    "df_results_context.groupby(FEATURE).agg(agg_dict).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect average performance per config value\n",
    "FEATURE = \"prompt\"\n",
    "df_results_context.groupby(FEATURE).agg(agg_dict).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect average performance per config value\n",
    "FEATURE = \"prompt\"\n",
    "df_results_context.groupby([\"model\", FEATURE], as_index=False).agg(agg_dict).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect average performance per config value\n",
    "FEATURE = \"example_selec\"\n",
    "df_results_context.groupby(FEATURE).agg(agg_dict).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best-performing config per model, per prompt persona\n",
    "\n",
    "# Get the index of best-performing config per model-prompt pair\n",
    "if \"kt\" in EXP_NAME:\n",
    "    metric = f\"{SPLIT} bal acc (KT)\"\n",
    "else:\n",
    "    metric = f\"{SPLIT} bal acc\"\n",
    "best_indices = df_results_context.groupby([\"model\"])[metric].idxmax()\n",
    "\n",
    "# Get the full rows for these best-performing configs\n",
    "best_configs = df_results_context.loc[best_indices]\n",
    "best_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best 3 configs per LLM\n",
    "(\n",
    "    df_results_context.groupby([\"model\"])\n",
    "    .apply(lambda x: x.nlargest(3, metric))\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-contextual models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_nocontext = df_results[df_results[\"num_examples\"] == 0]\n",
    "agg_dict = {f\"{SPLIT} f1 (macro)\": \"mean\", f\"{SPLIT} f1 (KT)\": \"mean\", \"llm correctness\": \"mean\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_results_nocontext.empty:\n",
    "    # inspect average performance per config value\n",
    "    FEATURE = \"model\"\n",
    "    df_results_nocontext.groupby(FEATURE).agg(agg_dict).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_results_nocontext.empty:\n",
    "    # inspect average performance per config value\n",
    "    FEATURE = \"temp\"\n",
    "    df_results_nocontext.groupby(FEATURE).agg(agg_dict).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_results_nocontext.empty:\n",
    "    # inspect average performance per config value\n",
    "    FEATURE = \"prompt\"\n",
    "    df_results_nocontext.groupby(FEATURE).agg(agg_dict).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_results_nocontext.empty:\n",
    "    # inspect average performance per config value\n",
    "    FEATURE = \"example_selec\"\n",
    "    df_results_nocontext.groupby(FEATURE).agg(agg_dict).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_results_nocontext.empty:\n",
    "    # get best-performing config per model, per prompt persona\n",
    "\n",
    "    # Get the index of best-performing config per model-prompt pair\n",
    "    best_indices = df_results_nocontext.groupby([\"model\", \"prompt\"])[f\"{SPLIT} f1 (macro)\"].idxmax()\n",
    "\n",
    "    # Get the full rows for these best-performing configs\n",
    "    best_configs = df_results_nocontext.loc[best_indices]\n",
    "    best_configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrices\n",
    "## LLM vs student performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # single config\n",
    "# config_id = \"qwen3:8b~T_0.0~SO_teacher~SP_replicate_teacher_avocado~EF_quotes~ES_studentid_random3\"  # TODO\n",
    "# if config_id not in config_ids:\n",
    "#     logger.error(f\"Config ID not available\", config_id=config_id)\n",
    "# else:\n",
    "#     preds_dict = get_llm_student_preds(\n",
    "#         exp_name=EXP_NAME,\n",
    "#         config_id=config_id,\n",
    "#         run_id=1,\n",
    "#         split=\"val\",\n",
    "#     )\n",
    "#     plot_llm_student_confusion(\n",
    "#         preds_dict,\n",
    "#         config_id=config_id,\n",
    "#         normalize=\"all\",\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # all configs\n",
    "# for config_id in config_ids:\n",
    "#     preds_dict = get_llm_student_preds(\n",
    "#         exp_name=EXP_NAME,\n",
    "#         config_id=config_id,\n",
    "#         run_id=1,\n",
    "#         split=\"val\",\n",
    "#         problem_type=PROBLEM_TYPE,\n",
    "#     )\n",
    "#     plot_llm_student_confusion(\n",
    "#         preds_dict,\n",
    "#         config_id=config_id,\n",
    "#         normalize=\"all\",\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # all configs\n",
    "# for config_id in config_ids:\n",
    "#     preds_dict = get_llm_student_preds(\n",
    "#         exp_name=EXP_NAME,\n",
    "#         config_id=config_id,\n",
    "#         run_id=1,\n",
    "#         split=\"val\",\n",
    "#         problem_type=PROBLEM_TYPE,\n",
    "#     )\n",
    "#     plot_kt_confusion(\n",
    "#         preds_dict,\n",
    "#         config_id=config_id,\n",
    "#         normalize=\"all\",\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # all configs\n",
    "# for config_id in config_ids:\n",
    "#     preds_dict = get_llm_student_preds(\n",
    "#         exp_name=EXP_NAME,\n",
    "#         config_id=config_id,\n",
    "#         run_id=1,\n",
    "#         split=\"val\",\n",
    "#         problem_type=PROBLEM_TYPE,\n",
    "#     )\n",
    "#     plot_level_correctness(\n",
    "#         preds_dict,\n",
    "#         problem_type=PROBLEM_TYPE,\n",
    "#         config_id=config_id,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributions\n",
    "## LLM vs student correctness\n",
    "### Contextual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_family=\"qwen3\"\n",
    "plot_llm_correctness_by_model(\n",
    "    df_results=df_results_context,\n",
    "    model_family=model_family,\n",
    "    exclude_models=[\"qwen3:1.7b\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_family=\"llama\"\n",
    "plot_llm_correctness_by_model(\n",
    "    df_results=df_results_context,\n",
    "    model_family=model_family,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_FAMILIES = [\"qwen3\", \"llama\"]\n",
    "EXCLUDE_MODELS = {\"qwen3\": [\"qwen3:1.7b\"], \"llama\": []}\n",
    "\n",
    "if PRINT_PAPER:\n",
    "    activate_latex(sans_serif=SANS_SERIF)\n",
    "    ########\n",
    "    # plot for every config_id\n",
    "    for llm_family in LLM_FAMILIES:\n",
    "\n",
    "        fname = os.path.join(\n",
    "            \"output\", EXP_NAME, \"figures\", f\"llm_correctness_{SPLIT}_{llm_family}.pdf\"\n",
    "        )\n",
    "        plot_llm_correctness_by_model(\n",
    "            df_results=df_results_context,\n",
    "            model_family=llm_family,\n",
    "            exclude_models=EXCLUDE_MODELS[llm_family],\n",
    "            savename=fname,\n",
    "        )\n",
    "    ########\n",
    "    deactivate_latex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_family=\"qwen3\"\n",
    "plot_llm_correctness_by_model(\n",
    "    df_results=df_results_nocontext,\n",
    "    model_family=model_family,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_family=\"llama\"\n",
    "plot_llm_correctness_by_model(\n",
    "    df_results=df_results_nocontext,\n",
    "    model_family=model_family,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per config value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tools.utils import ensure_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_llm_correctness(\n",
    "    df_results: pd.DataFrame,\n",
    "    savename: Optional[str] = None,\n",
    ") -> None:\n",
    "    \"\"\"Plot LLM correctness over all configurations\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_results : pd.DataFrame\n",
    "        DataFrame containing the results\n",
    "    model_family : str\n",
    "        Name of the model family\n",
    "    savename : Optional[str], optional\n",
    "        Name to save plot to, by default None\n",
    "    \"\"\"\n",
    "    df = df_results[[\"model\", \"llm correctness\", \"student correctness\"]]\n",
    "\n",
    "    student_correctness_scalar = df[\"student correctness\"].values[0]\n",
    "\n",
    "    _, ax = plt.subplots()\n",
    "\n",
    "    # density\n",
    "    sns.kdeplot(data=df, x=\"llm correctness\", fill=True, ax=ax, palette=\"tab10\")\n",
    "\n",
    "    ax.axvline(student_correctness_scalar, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "    ax.set_xlabel(\"LLM answer correctness\")\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    ax.grid(True, linestyle=\"--\")\n",
    "    # ax.legend_.set_title(None)\n",
    "    # get ticks in sans-serif if sans-serif is used\n",
    "    ax.xaxis.get_major_formatter()._usetex = False\n",
    "    ax.yaxis.get_major_formatter()._usetex = False\n",
    "\n",
    "    if savename is not None:\n",
    "        plt.tight_layout()\n",
    "        ensure_dir(os.path.dirname(savename))\n",
    "        plt.savefig(savename)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correctness_by_config(\n",
    "    df_results: pd.DataFrame,\n",
    "    config_value: str,\n",
    "    savename: Optional[str] = None,\n",
    "    hyperparamkey2legend: Optional[dict] = None,\n",
    "    hyperparamvalue2legend: Optional[dict] = None,\n",
    ") -> None:\n",
    "    \"\"\"Plot LLM correctness over all configurations\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_results : pd.DataFrame\n",
    "        DataFrame containing the results\n",
    "    model_family : str\n",
    "        Name of the model family\n",
    "    savename : Optional[str], optional\n",
    "        Name to save plot to, by default None\n",
    "    \"\"\"\n",
    "    df = df_results.copy()\n",
    "    df[\"prompt_persona\"] = df_results[\"prompt\"].str.extract(r\"^([^_]+)\")\n",
    "    df = df[[\"model\", config_value, \"llm correctness\", \"student correctness\"]]\n",
    "    # use map to replace values in the config column with their corresponding legend labels\n",
    "    if config_value in hyperparamvalue2legend:\n",
    "        df[config_value] = df[config_value].map(hyperparamvalue2legend[config_value])\n",
    "\n",
    "    student_correctness_scalar = df[\"student correctness\"].values[0]\n",
    "\n",
    "    _, ax = plt.subplots()\n",
    "\n",
    "    # density\n",
    "    sns.kdeplot(data=df, x=\"llm correctness\", hue=config_value, fill=True, ax=ax, palette=\"tab10\")\n",
    "\n",
    "    ax.axvline(student_correctness_scalar, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "    ax.set_xlabel(\"LLM answer correctness\")\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    ax.grid(True, linestyle=\"--\")\n",
    "    # update legend title\n",
    "    if hyperparamkey2legend is not None:\n",
    "        hyperparam_print = hyperparamkey2legend.get(config_value, config_value)\n",
    "    else:\n",
    "        hyperparam_print = config_value\n",
    "    ax.legend_.set_title(hyperparam_print)\n",
    "    # get ticks in sans-serif if sans-serif is used\n",
    "    ax.xaxis.get_major_formatter()._usetex = False\n",
    "    ax.yaxis.get_major_formatter()._usetex = False\n",
    "\n",
    "    if savename is not None:\n",
    "        plt.tight_layout()\n",
    "        ensure_dir(os.path.dirname(savename))\n",
    "        plt.savefig(savename)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "HYPERPARAMKEY2LEGEND_DICT = {\n",
    "    \"temp\": \"Temperature\",\n",
    "    \"prompt\": \"Prompt\",\n",
    "    \"example_selec\": \"Example selection\",\n",
    "    \"num_examples\": \"Number of examples\",\n",
    "    \"prompt_persona\": \"Prompt persona\",\n",
    "}\n",
    "HYPERPARAMVALUE2LEGEND_DICT = {\n",
    "    \"prompt_persona\": {\"teacher\": \"Teacher\", \"student\": \"Student\"},\n",
    "    \"example_selec\": {\"miscon_studentid_random\": \"Random\", \"miscon_studentid_kc_exact\": \"Knowledge Concept\"}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_HUE = \"prompt_persona\"\n",
    "plot_correctness_by_config(\n",
    "    df_results=df_results_context,\n",
    "    config_value=CONFIG_HUE,\n",
    "    hyperparamkey2legend=HYPERPARAMKEY2LEGEND_DICT,\n",
    "    hyperparamvalue2legend=HYPERPARAMVALUE2LEGEND_DICT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_HUE = \"temp\"\n",
    "plot_correctness_by_config(\n",
    "    df_results=df_results_context,\n",
    "    config_value=CONFIG_HUE,\n",
    "    hyperparamkey2legend=HYPERPARAMKEY2LEGEND_DICT,\n",
    "    hyperparamvalue2legend=HYPERPARAMVALUE2LEGEND_DICT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_HUE = \"num_examples\"\n",
    "plot_correctness_by_config(\n",
    "    df_results=df_results_context,\n",
    "    config_value=CONFIG_HUE,\n",
    "    hyperparamkey2legend=HYPERPARAMKEY2LEGEND_DICT,\n",
    "    hyperparamvalue2legend=HYPERPARAMVALUE2LEGEND_DICT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_HUE = \"example_selec\"\n",
    "plot_correctness_by_config(\n",
    "    df_results=df_results_context,\n",
    "    config_value=CONFIG_HUE,\n",
    "    hyperparamkey2legend=HYPERPARAMKEY2LEGEND_DICT,\n",
    "    hyperparamvalue2legend=HYPERPARAMVALUE2LEGEND_DICT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_llm_correctness(\n",
    "    df_results=df_results_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_HUES = [\"prompt_persona\", \"temp\", \"example_selec\", \"num_examples\"]\n",
    "HYPERPARAMKEY2LEGEND_DICT = {\n",
    "    \"temp\": \"Temperature\",\n",
    "    \"prompt\": \"Prompt\",\n",
    "    \"example_selec\": \"Example selection\",\n",
    "    \"num_examples\": \"Number of examples\",\n",
    "    \"prompt_persona\": \"Prompt persona\",\n",
    "}\n",
    "HYPERPARAMVALUE2LEGEND_DICT = {\n",
    "    \"prompt_persona\": {\"teacher\": \"Teacher\", \"student\": \"Student\"},\n",
    "    \"example_selec\": {\n",
    "        \"miscon_studentid_random\": \"Random\",\n",
    "        \"miscon_studentid_kc_exact\": \"Knowledge Concept\",\n",
    "    },\n",
    "}\n",
    "\n",
    "if PRINT_PAPER:\n",
    "    activate_latex(sans_serif=SANS_SERIF)\n",
    "    ########\n",
    "    # plot over all configs\n",
    "    fname = os.path.join(\n",
    "            \"output\", EXP_NAME, \"figures\", f\"llm_correctness_{SPLIT}_all.pdf\"\n",
    "        )\n",
    "    plot_llm_correctness(\n",
    "        df_results=df_results_context,\n",
    "        savename=fname,\n",
    "    )\n",
    "\n",
    "    # plot for every config hue\n",
    "    for config_hue in CONFIG_HUES:\n",
    "\n",
    "        fname = os.path.join(\n",
    "            \"output\", EXP_NAME, \"figures\", f\"llm_correctness_{SPLIT}_{config_hue}.pdf\"\n",
    "        )\n",
    "        plot_correctness_by_config(\n",
    "            df_results=df_results_context,\n",
    "            config_value=config_hue,\n",
    "            hyperparamkey2legend=HYPERPARAMKEY2LEGEND_DICT,\n",
    "            hyperparamvalue2legend=HYPERPARAMVALUE2LEGEND_DICT,\n",
    "            savename=fname,\n",
    "        )\n",
    "    ########\n",
    "    deactivate_latex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
