DONE:





TODO:
- Kate's experiments
    - initial experiments use the "random" example selector -> room for improvement with smarter selection (studentid_semantic, studentid_recency)
    - initial experiments use 3 example interactions -> try 1, 3, 5, 10
    - (initial experiments use temperature 0.0 and 0.6 -> try larger values, above 1.0)
- Example selectors: log the number of generated examples (might be lower than the requested number if the student history is too short)
- Find way to create yaml configurations automatically instead of manually
- Models
    - Add Gemini 2.5 Pro and 2.5 Flash (check LangChain wrapper)
    - @Kate: send Gemini API key to Arthur
    - Look into GPT-4.1 and GPT-4.5
    - (Add Mistral Small 3.1 -> can we run this locally? If not, need to use the supercomputer)
- Embeddings
    - Add Gemini embedding
- Structured output
    - Can we omit asking for explanations? This might save costs because less output tokens are generated.
    - (Can ask for probability distribution over the answer options, i.e., softmax output -> analyze uncertainty)
- Misconceptions
    - For the target questions, we would like to add general misconceptions in the questions using this particular knowledge concept (DBE-KT22 has knowledge concepts per question). This can be computed separately from the normal runtime.
    - First step: only ask for misconceptions based on the question input
    - Second step: ask for misconceptions based on the question input and the answer options
    - => Show the general knowledge concept misconceptions, together with the target question at runtime.