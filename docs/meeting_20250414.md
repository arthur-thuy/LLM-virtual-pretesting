DONE:
- example formatter with quotes?
- Not clear what 6 question to delete. Not really less answered, too easy/difficult, not related to number of answer options.
    - id 24,25,26,27 refers to figure we don't have
    - id 91 also refers to figure?
- DBE-KT22:
    - what to do with students that answered questions multiple times? keep first!
    - use IRT estimation to get ground truth in DBE-KT22!
- splitting: split on question_id! (not interactions)
- refactor prompt building (bring message building inside modular function) to make it more flexible.
- changed StudentIDSemanticExampleSelector to only include interactions that were done before Q of interest
- new example selector: studentid_recency (k most recent question-answer records)
- changed example selectors: student-specific selectors now only get interactions from the past (not future)


TODO:
- why is discrimination range [+1, +1] in IRT estimation?
- How do we handle roleplaying? How many times do we ask an LLM response per test set question?
- When filtering in student history, do we only show question-answer records from past interactions?
- Do we start at calculating final difficulty with IRT or do we first focus on replicating student behaviour?
- configurations:
    - can we define which yacs registry options to use and then builds all the combinations automatically?
    - do we have some kind of prompt engineering baseline score (e.g., Luca's reference prompt from the EMNLP paper)?
    - number of configurations: 256 -> how do we handle this?
        - example formatter: 2
        - example selector: 4
        - models: 4
        - prompt structures: 4
        - structured outputs: 2

